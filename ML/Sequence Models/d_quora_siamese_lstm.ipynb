{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"d_quora_siamese_lstm.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"z1FU2MQ5PcwJ"},"source":["# Big Data Content Analytics - AUEB\n","\n","## Siamese LSTM Networks Implementation \n","\n","* Lab Assistant: George Perakis\n","* Email: gperakis[at]aeub.gr | perakisgeorgios[at]gmail.com"]},{"cell_type":"markdown","metadata":{"id":"Efacm96xPcwK"},"source":["### Import Modules"]},{"cell_type":"code","metadata":{"id":"hB7lGDThPcwM"},"source":["import re\n","from pathlib import Path\n","from time import time\n","\n","import tensorflow.python.keras.backend as K\n","import numpy as np\n","import pandas as pd\n","from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.python.keras.layers import Input, Embedding, LSTM, Lambda, Dense\n","from tensorflow.python.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.python.keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from tqdm import tqdm\n","\n","tqdm.pandas()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lUH5QEcAPcwS"},"source":["### ETL related functions"]},{"cell_type":"code","metadata":{"code_folding":[0],"id":"1YfZMCXtPcwT"},"source":["def clean_text(text):\n","    \"\"\"\n","    Pre process and convert texts to a list of words\n","    :param text:\n","    :return:\n","    \"\"\"\n","\n","    text = str(text)\n","    text = text.lower()\n","\n","    # Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJKrOwzTPcwX"},"source":["### Pre-trained Embeddings (GLOVE) related functions"]},{"cell_type":"code","metadata":{"code_folding":[0],"id":"cKt1YWlOPcwY"},"source":["def load_glove_embeddings(glove_dir: Path,\n","                          dim: int = 100) -> dict:\n","    \"\"\"\n","\n","    :param dim: The embeddings size (dimensions)\n","    :return:\n","    \"\"\"\n","    #     print('Loading word vectors')\n","\n","    embed_index = dict()  # We create a dictionary of word -> embedding\n","\n","    fname = glove_dir.joinpath(f'glove.6B.{dim}d.txt')\n","\n","    f = open(fname, encoding=\"utf8\")  # Open file\n","\n","    # In the dataset, each line represents a new word embedding\n","    # The line starts with the word and the embedding values follow\n","    for line in tqdm(f, desc='Loading Embeddings', unit='word'):\n","        values = line.split()\n","        # The first value is the word, the rest are the values of the embedding\n","        word = values[0]\n","        # Load embedding\n","        embedding = np.asarray(values[1:], dtype='float32')\n","\n","        # Add embedding to our embedding dictionary\n","        embed_index[word] = embedding\n","    f.close()\n","\n","    print(f'Found %s word vectors. {len(embed_index)}')\n","\n","    return embed_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"code_folding":[0],"id":"njpBkmGdPcwd"},"source":["def create_embeddings_matrix(emb_index: dict,\n","                             max_words: int,\n","                             tokenizer: Tokenizer,\n","                             emb_dim: int = 100) -> np.ndarray:\n","    \"\"\"\n","\n","    :param max_words:\n","    :param emb_index: Embeddings Index\n","    :param tokenizer: Keras fitted tokenizer.\n","    :param emb_dim: Embeddings dimension.\n","    :return: A matrix of shape (nb_words, emb_dim) containing the globe embeddings.\n","    \"\"\"\n","    assert emb_dim in [50, 100, 200, 300]\n","\n","    # Create a matrix of all embeddings\n","    # (stacking=concatenating all the vectors)\n","    all_embs = np.stack(emb_index.values())  # .values() gets the all the arrays from the keys\n","\n","    # Calculate mean\n","    emb_mean = all_embs.mean()\n","    # Calculate standard deviation\n","    emb_std = all_embs.std()\n","\n","    print(f\"Embeddings AVG: {emb_mean} | STD: {emb_std}\")\n","\n","    # We can now create an embedding matrix holding all word vectors.\n","\n","    word_index = tokenizer.word_index\n","\n","    # How many words are there actually. Because we may have requested X most common tokens\n","    # and the total tokens are X/2\n","    nb_words = min(max_words, len(word_index))\n","\n","    # Create a random matrix with the same mean and std as the embeddings\n","\n","    embedding_matrix = np.random.normal(emb_mean,  # mean\n","                                        emb_std,  # std\n","                                        (nb_words, emb_dim)  # shape of the matrix\n","                                        )\n","\n","    # The vectors need to be in the same position as their index.\n","    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n","\n","    # Loop over all words in the word index\n","    for word, i in word_index.items():  # .items() return a tuple with (word, word_index)\n","\n","        # If we are above the amount of words we want to use we do nothing\n","        if i >= max_words:\n","            continue\n","\n","        # Get the embedding vector for the word\n","        embedding_vector = emb_index.get(word)\n","\n","        # If there is an embedding vector, put it in the embedding matrix\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uI3LW7xfPcwh"},"source":["### Siamese model implementation related functions"]},{"cell_type":"code","metadata":{"code_folding":[0],"id":"HxiBKr0ZPcwi"},"source":["def exponent_neg_manhattan_distance(left, right):\n","    \"\"\"\n","    Helper function for the similarity estimate of the LSTMs outputs\n","    :param left:\n","    :param right:\n","    :return:\n","    \"\"\"\n","    return K.exp(-K.sum(K.abs(left - right),\n","                        axis=1,\n","                        keepdims=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QfNsb6xDWqDZ"},"source":["<img src=\"https://miro.medium.com/max/900/1*SZM2gDnr-OTx9ytVKQEuOg.png\">"]},{"cell_type":"code","metadata":{"id":"sNSwZQtaPcwl"},"source":["def build_model(max_seq_len,\n","                emb_matrix,\n","                emb_dim,\n","                grad_clip_norm,\n","                n_hidden: int = 50) -> Model:\n","    \"\"\"\n","\n","    :param max_seq_len:\n","    :param emb_matrix:\n","    :param emb_dim:\n","    :param grad_clip_norm:\n","    :param n_hidden:\n","    :return:\n","    \"\"\"\n","    # The visible layer\n","    left_input = Input(shape=(max_seq_len,),\n","                       dtype='int32',\n","                       name='left_input')\n","\n","    right_input = Input(shape=(max_seq_len,),\n","                        dtype='int32',\n","                        name='right_input')\n","\n","    embedding_layer = Embedding(len(emb_matrix),\n","                                emb_dim,\n","                                weights=[emb_matrix],\n","                                input_length=max_seq_len,\n","                                trainable=False,\n","                                name='emb_layer')\n","\n","    # Embedded version of the inputs\n","    encoded_left = embedding_layer(left_input)\n","    encoded_right = embedding_layer(right_input)\n","\n","    # Since this is a siamese network, both sides share the same LSTM\n","    shared_lstm = LSTM(n_hidden, name='shared_lstm')\n","\n","    left_output = shared_lstm(encoded_left)\n","    right_output = shared_lstm(encoded_right)\n","\n","    # # Calculates the distance as defined by the MaLSTM model\n","    # malstm_distance = Lambda(function=lambda tensors: exponent_neg_manhattan_distance(tensors[0],\n","    #                                                                                   tensors[1]),\n","    #                          output_shape=lambda x: (x[0][0], 1))([left_output,\n","    #                                                                right_output])\n","    # Add a customized layer to compute the absolute difference between the encodings\n","\n","    l1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n","\n","    l1_distance = l1_layer([left_output, right_output])\n","\n","    output = Dense(units=1, activation='sigmoid',\n","                   name='output')(l1_distance)\n","\n","    # Pack it all up into a model\n","    malstm = Model(inputs=[left_input,\n","                           right_input],\n","                   outputs=[output])\n","\n","    # Ada-delta optimizer, with gradient clipping by norm\n","    optimizer = Adam(clipnorm=grad_clip_norm)\n","\n","    malstm.compile(loss='binary_crossentropy',\n","                   optimizer=optimizer,\n","                   metrics=['accuracy'])\n","\n","    print(malstm.summary())\n","\n","    return malstm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"COsjaJT3Pcwr"},"source":["### Configuration (Hyper-parameter) functions"]},{"cell_type":"code","metadata":{"id":"G4Lc5KtqPcws"},"source":["class Config:\n","    MAX_FEATURES = 20_000\n","\n","    # Model variables\n","    N_HIDDEN = 50\n","    GRADIENT_CLIPPING_NORM = 1.20\n","    BATCH_SIZE = 128\n","    NB_EPOCHS = 25\n","    EMB_DIMENSIONS = 300"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jaXXJ11Y4N4"},"source":["#  download glove embeddings\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","#  unzip clove embeddings \n","!unzip glove.6B.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsnI1F2VaLZM"},"source":["#  extract quora train dataset\n","! unzip train.csv.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1JoCYOMIPcww"},"source":["## Example\n","\n","### Load Data"]},{"cell_type":"code","metadata":{"id":"tTm2mHIqPcwx"},"source":["data = pd.read_csv('train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JoFADF0VPcw0"},"source":["print(data.iloc[5]['question1'])\n","data.iloc[5]['question2']\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jaVafwgfPcw6"},"source":["#### Clean Questions"]},{"cell_type":"code","metadata":{"id":"lDKDvtW5Pcw6"},"source":["data['question1'] = data['question1'].progress_apply(clean_text)\n","data['question2'] = data['question2'].progress_apply(clean_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uNoLDVZPcw-"},"source":["X = data[['question1', 'question2']]\n","\n","y = data['is_duplicate']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xKJDKMT-PcxC"},"source":["#### Stratified Split"]},{"cell_type":"code","metadata":{"id":"F7astoB9PcxC"},"source":["# We will use this object to split at first in Train - Test in a stratified manner.\n","val_sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n","\n","X_train, X_val, y_train, y_val = None, None, None, None\n","\n","for train_index, val_index in val_sss.split(X, y):\n","    X_train, X_val = X.loc[train_index], X.loc[val_index]\n","    y_train, y_val = y.loc[train_index], y.loc[val_index]\n","\n","print('Train')\n","print(y_train.value_counts() / y_train.count())\n","print('Validation')\n","print(y_val.value_counts() / y_val.count())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SaAp9KQEPcxG"},"source":["### Create Tokenizer"]},{"cell_type":"code","metadata":{"id":"LnBTb3UvPcxG"},"source":["max_features = Config.MAX_FEATURES\n","\n","tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n","\n","# fitting on the train dataset only\n","tokenizer.fit_on_texts(list(X_train['question1']) + list(X_train['question2']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ts0pOmMUPcxK"},"source":["#### Data Vectorization"]},{"cell_type":"code","metadata":{"id":"GpGjaBQYPcxL"},"source":["X_train['question1_seqs'] = tokenizer.texts_to_sequences(X_train['question1'])\n","X_train['question2_seqs'] = tokenizer.texts_to_sequences(X_train['question2'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DN2MyvJgPcxP"},"source":["X_val['question1_seqs'] = tokenizer.texts_to_sequences(X_val['question1'])\n","X_val['question2_seqs'] = tokenizer.texts_to_sequences(X_val['question2'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1EhTBfVkPcxS"},"source":["all_train_lengths = list(X_train.question1_seqs.apply(len)) + list(X_train.question1_seqs.apply(len))\n","\n","max_len = int(np.percentile(all_train_lengths, q=90))\n","print(f'Max Length: {max_len}')\n","\n","print(X_train.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFHK6ZtTPcxW"},"source":["X_train_q1 = pad_sequences(X_train['question1_seqs'],\n","                           maxlen=max_len,\n","                           padding='post',\n","                           truncating='post')\n","\n","X_train_q2 = pad_sequences(X_train['question2_seqs'],\n","                           maxlen=max_len,\n","                           padding='post',\n","                           truncating='post')\n","\n","X_val_q1 = pad_sequences(X_val['question1_seqs'],\n","                         maxlen=max_len,\n","                         padding='post',\n","                         truncating='post')\n","\n","X_val_q2 = pad_sequences(X_val['question2_seqs'],\n","                         maxlen=max_len,\n","                         padding='post',\n","                         truncating='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2D4kWkyAPcxa"},"source":["# Make sure everything is ok\n","assert X_train_q1.shape == X_train_q2.shape\n","assert X_val_q1.shape == X_val_q2.shape\n","\n","assert len(X_train_q1) == len(y_train)\n","assert len(X_train_q2) == len(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VaPeinMpPcxd"},"source":["### Load pre-trained embeddings"]},{"cell_type":"code","metadata":{"id":"UuRvnzgXPcxe"},"source":["gl_dir = Path('.')\n","\n","glove_embeddings = load_glove_embeddings(glove_dir=gl_dir,\n","                                         dim=Config.EMB_DIMENSIONS)\n","\n","emb_matrix = create_embeddings_matrix(emb_index=glove_embeddings,\n","                                      max_words=max_features,\n","                                      tokenizer=tokenizer,\n","                                      emb_dim=Config.EMB_DIMENSIONS)\n","print(emb_matrix.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JG7GxV2CPcxh"},"source":["### Siamese LSTM Model\n","\n","#### Build Model"]},{"cell_type":"code","metadata":{"id":"nJK2DzUYPcxh"},"source":["model = build_model(max_seq_len=max_len,\n","                    emb_matrix=emb_matrix,\n","                    emb_dim=Config.EMB_DIMENSIONS,\n","                    grad_clip_norm=Config.GRADIENT_CLIPPING_NORM,\n","                    n_hidden=Config.N_HIDDEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NEvtpjSCPcxk"},"source":["#### Load previous weigths"]},{"cell_type":"code","metadata":{"id":"UQm8lj3uPcxl"},"source":["weights_fname = 'quora_siamese_lstm_weights.h5'\n","\n","try:\n","    model.load_weights(weights_fname)\n","except:\n","    print('Pre-trained weights not found. Fitting from start')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrMovpXcPcxo"},"source":["#### Create Callbacks"]},{"cell_type":"code","metadata":{"id":"WYDsgRUcPcxp"},"source":["monitor_metric = 'val_loss'\n","\n","callbacks = [\n","    EarlyStopping(monitor=monitor_metric,\n","                  patience=3,\n","                  verbose=1,\n","                  restore_best_weights=True),\n","\n","    ModelCheckpoint(filepath=weights_fname,\n","                    monitor=monitor_metric,\n","                    verbose=1,\n","                    save_best_only=True,\n","                    save_weights_only=True)\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8kIEM-NPcxs"},"source":["#### Fit Model"]},{"cell_type":"code","metadata":{"id":"YqFkbGwRPcxt"},"source":["s = time()\n","history = model.fit({'left_input': X_train_q1,\n","                     'right_input': X_train_q2},\n","                    y_train,\n","                    batch_size=Config.BATCH_SIZE,\n","                    epochs=Config.NB_EPOCHS,\n","                    validation_data=(\n","                        {'left_input': X_val_q1,\n","                         'right_input': X_val_q2},\n","                        y_val),\n","                    verbose=1,\n","                    callbacks=callbacks)\n","\n","duration = time() - s\n","print(f\"Training time finished. Duration {duration} secs\")"],"execution_count":null,"outputs":[]}]}