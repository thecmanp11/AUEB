{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Content Analytics - AUEB\n",
    "\n",
    "## Introduction to Word Embeddings and Embeddings Layers\n",
    "\n",
    "* Lab Assistant: George Perakis\n",
    "* Email: gperakis[at]aeub.gr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset \n",
    "#### Movies reviews dataset from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset from here\n",
    "# https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "# Unzip dataset in the same folder.\n",
    "\n",
    "# !unzip 134715_320111_bundle_archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_imdb_data(fname: str ='IMDB Dataset.csv'):\n",
    "    \n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    # We will use this mapping for tranforming the sentiment labels to integers\n",
    "    label_map = {'negative': 0, 'positive': 1}\n",
    "\n",
    "\n",
    "    reviews = df['review']\n",
    "    sentiment = df['sentiment'].map(label_map)\n",
    "    \n",
    "    return reviews, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the texts and the labels\n",
    "texts, labels = load_imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels), len(texts))\n",
    "\n",
    "print(np.mean(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive review\n",
    "idx = 24001\n",
    "print(f'Label: {labels[idx]}')\n",
    "\n",
    "print(texts[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative review\n",
    "idx = 49998\n",
    "print(f'Label: {labels[idx]}')\n",
    "\n",
    "print(texts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 15_000  # We will only consider the 15K most used words in this dataset\n",
    "\n",
    "# Setting up Keras tokenizer\n",
    "reviews_tokenizer = Tokenizer(num_words=max_words, lower=True, oov_token='<OOV>')\n",
    "\n",
    "reviews_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is like the .fit() that we call we using Scikit-learn and Count-Vectorizer\n",
    "reviews_tokenizer.fit_on_texts(texts)  # Generate tokens by counting frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is like the .transform() that we call we using Scikit-learn and Count-Vectorizer.\n",
    "# The major difference is that it turns text into sequence of numbers. NOT one-hot-encoding\n",
    "sequences = reviews_tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizers word index is a dictionary that maps each word to a number.\n",
    "# You can see that words that are frequently used in discussions about\n",
    "# movies have a lower token number.\n",
    "word_index = reviews_tokenizer.word_index\n",
    "\n",
    "for w in ['the', 'movie', 'generator']:\n",
    "    print(f'Token for the word \"{w}\": {word_index[w]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 words of the sequence tokenized\n",
    "print(sequences[24002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To proceed, we now have to make sure that all text sequences we feed into the model\n",
    "# have the same length.\n",
    "\n",
    "# We can do this with Keras pad sequences tool.\n",
    "# It cuts of sequences that are too long and adds zeros to sequences that are too short.\n",
    "\n",
    "# Make all sequences 100 words long\n",
    "maxlen = 100\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# We have 25K, 100 word sequences now\n",
    "print('New data shape: {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data shuffling and splitting in train-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can turn all data into proper training and validation data.\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling data\n",
    "\n",
    "# Using the length of the texts we create indexes\n",
    "# numpy's range() function. eg array([0, 1, 2, 3, 4, 5])\n",
    "indices = np.arange(data.shape[0])  \n",
    "\n",
    "# We shuffle the indices on the fly, eg: array([3, 0, 1, 4, 2, 5])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]  # we get the shuffled texts\n",
    "labels = labels[indices]  # and the shuffled sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = 40_000  # We will be training on 40K samples\n",
    "\n",
    "# Split data\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples:]\n",
    "y_val = labels[training_samples:]\n",
    "\n",
    "print(f'X train shape: {x_train.shape}')\n",
    "print(f'y train shape: {y_train.shape}')\n",
    "\n",
    "print(f'X val shape: {x_val.shape}')\n",
    "print(f'y val shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "1. Words and word tokens are **categorical** features.\n",
    "1. As such, we **can not directly feed them into the neural net**.\n",
    "1. Just because a word has a larger token value, it **does not express a higher value** in any way. It is just a **different category**.\n",
    "1. We have already dealt with categorical data by turning it into **one-hot-encoded vectors** (by using Scikit-Learn's CountVectorizer()\n",
    "1. But for words, this is impractical. Since our vocabulary is **10,000 words**, each vector would contain **10,000 numbers** which are all zeros except for one. This is highly inefficient. Instead we will use an embedding.\n",
    "\n",
    "1. **Embeddings** also turn **categorical data into vectors**.\n",
    "1. Instead of creating a one hot vector, we create a vector in which **all elements are numbers** (dense vectors)\n",
    "\n",
    "1. In practice, **embeddings work like a look up table**. For each token, they store a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In practice it looks like this:\n",
    "\n",
    "1. We have to specify how large we want the word vectors to be.\n",
    "2.  A 50 dimensional vector is able to capture good embeddings even for quite large vocabularies.\n",
    "3. We also have to specify:\n",
    "  * for **how many words** we want embeddings\n",
    "  * **How long** our sequences are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Model with self-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_model_1(maximum_words, \n",
    "                  max_seq_len, \n",
    "                  emb_dim: int = 50):\n",
    "    \"\"\"\n",
    "    Keras model with self trained Embedding Layer\n",
    "    \n",
    "    :param maximum_words: Total number of words to be used by the model\n",
    "    :param max_seq_len: The sequence length for each text (total number of tokens)\n",
    "    :param emb_dim: The size of the embeddings vector.\n",
    "    :return: A sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    seq_model = Sequential()\n",
    "    seq_model.add(Embedding(input_dim=maximum_words,\n",
    "                            output_dim=emb_dim,\n",
    "                            embeddings_initializer='uniform',\n",
    "                            input_length=max_seq_len))\n",
    "\n",
    "    seq_model.add(Flatten())\n",
    "\n",
    "    seq_model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    seq_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(seq_model.summary())\n",
    "\n",
    "    seq_model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "# I've already created a function that wraps up the model. Use this if you want\n",
    "# model = build_model_1(embedding_dim= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(input_dim=max_words,\n",
    "                      output_dim=embedding_dim,\n",
    "                      embeddings_initializer='uniform',\n",
    "                      mask_zero=True,\n",
    "                      input_length=maxlen))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "\n",
    "model_1.add(Dense(32, activation='relu'))\n",
    "\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model_1.summary())\n",
    "\n",
    "# You can see that the Embedding layer has 500,000 trainable parameters,\n",
    "# that is 50 parameters for each of the 10K words.\n",
    "\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1.fit(x_train,\n",
    "                      y_train,\n",
    "                      epochs=20,\n",
    "                      batch_size=64,\n",
    "                      validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOS\n",
    "\n",
    "* Note that training your own embeddings are **prone to over fitting**.\n",
    "\n",
    "* As you can see our model achieves 100% accuracy **on the training set** but only around  83-84% accuracy on the **validation set**.\n",
    "* This is a clear sign of over fitting\n",
    "\n",
    "* In practice it is therefore **quite rare to train new embeddings** unless you have a **massive dataset**. More commonly, pre-trained embeddings are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove Embeddings\n",
    "\n",
    "* https://nlp.stanford.edu/projects/glove/\n",
    "* We will use the following small pretrained-embedding dataset\n",
    "* http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "After downloading the GloVe embeddings from the GloVe website we can load them into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(dim: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Function that loads glove embeddings. \n",
    "\n",
    "    :param dim: The embeddings size (dimensions)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('Loading word vectors')\n",
    "\n",
    "    embed_index = dict()  # We create a dictionary of word -> embedding\n",
    "\n",
    "    fname = 'glove.6B.{}d.txt'.format(dim)\n",
    "\n",
    "    f = open(fname, encoding=\"utf8\")  # Open file\n",
    "\n",
    "    # In the dataset, each line represents a new word embedding\n",
    "    # The line starts with the word and the embedding values follow\n",
    "    for line in tqdm(f, desc='Loading Embeddings', unit='word'):\n",
    "        values = line.split()\n",
    "        # The first value is the word, the rest are the values of the embedding\n",
    "        word = values[0]\n",
    "        # Load embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        # Add embedding to our embedding dictionary\n",
    "        embed_index[word] = embedding\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embed_index))\n",
    "\n",
    "    return embed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300  # We now use larger embeddings\n",
    "\n",
    "embeddings_index = load_glove_embeddings(dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index['the'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not all words that are in our IMDB vocabulary might be in the GloVe embedding though.\n",
    "* For missing words it is wise to use **random embeddings** with the **same mean** and **standard deviation** as the GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings_matrix(emb_index: dict,\n",
    "                             tokenizer: Tokenizer,\n",
    "                             emb_dim: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "\n",
    "    :param emb_index: Embeddings Index\n",
    "    :param tokenizer: Keras fitted tokenizer.\n",
    "    :param emb_dim: Embeddings dimension.\n",
    "    :return: A matrix of shape (nb_words, emb_dim) containing the globe embeddings.\n",
    "    \"\"\"\n",
    "    assert emb_dim in [50, 100, 200, 300]\n",
    "\n",
    "    # Create a matrix of all embeddings\n",
    "    # (stacking=concatenating all the vectors)\n",
    "    all_embs = np.stack(emb_index.values())  # .values() gets the all the arrays from the keys\n",
    "\n",
    "    # Calculate mean\n",
    "    emb_mean = all_embs.mean()\n",
    "    # Calculate standard deviation\n",
    "    emb_std = all_embs.std()\n",
    "\n",
    "    print(\"Embeddings AVG: {} | STD: {}\".format(emb_mean, emb_std))\n",
    "\n",
    "    # We can now create an embedding matrix holding all word vectors.\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # How many words are there actually. Because we may have requested X most common tokens\n",
    "    # and the total tokens are X/2\n",
    "    nb_words = min(max_words, len(word_index))\n",
    "\n",
    "    # Create a random matrix with the same mean and std as the embeddings\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean,  # mean\n",
    "                                        emb_std,  # std\n",
    "                                        (nb_words, emb_dim)) # shape of the matrix\n",
    "\n",
    "    # The vectors need to be in the same position as their index.\n",
    "    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # Loop over all words in the word index\n",
    "    for word, i in word_index.items():  # .items() return a tuple with (word, word_index)\n",
    "\n",
    "        # If we are above the amount of words we want to use we do nothing\n",
    "        if i >= max_words:\n",
    "            continue\n",
    "\n",
    "        # Get the embedding vector for the word\n",
    "        embedding_vector = emb_index.get(word)\n",
    "\n",
    "        # If there is an embedding vector, put it in the embedding matrix\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            counter += 1\n",
    "    \n",
    "    print(f'Found {counter} pre-trained embeddings out of {nb_words}')\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embeddings_matrix(emb_index=embeddings_index,\n",
    "                                            tokenizer=reviews_tokenizer,\n",
    "                                            emb_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "* This **embedding matrix** can be used **as weights** for the **embedding** layer.\n",
    "* This way, the **embedding layer** uses the **pre-trained GloVe weights** instead of random ones.\n",
    "* We can also set the embedding layer to **NOT trainable**. This means, Keras won't change the  weights of the embeddings while training which makes sense since our embeddings are already trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model with Pre-Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_model_with_glove_embeddings(maximum_words,\n",
    "                                      emb_dim,\n",
    "                                      max_seq_len,\n",
    "                                      emb_matrix):\n",
    "    \"\"\"\n",
    "    This function builds a Keras model with pre-trained word embeddings\n",
    "\n",
    "    :param maximum_words: Total number of words to be used by the model\n",
    "    :param emb_dim: The size of the embeddings vector.\n",
    "    :param max_seq_len: The sequence length for each text (total number of tokens)\n",
    "    :param emb_matrix: The pretrained glove embedding matrix to be used as weights.\n",
    "    :return: a keras sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    seq_model = Sequential()\n",
    "    \n",
    "    seq_model.add(Embedding(input_dim=maximum_words,\n",
    "                            output_dim=emb_dim,\n",
    "                            input_length=max_seq_len,\n",
    "                            weights=[emb_matrix],\n",
    "                            trainable=False))\n",
    "\n",
    "    seq_model.add(Flatten())\n",
    "    seq_model.add(Dense(32, activation='relu'))\n",
    "    seq_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(seq_model.summary())\n",
    "\n",
    "    # Notice that we now have far fewer trainable parameters.\n",
    "    seq_model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(max_words,\n",
    "                      embedding_dim,\n",
    "                      input_length=maxlen,\n",
    "                      weights=[embedding_matrix],\n",
    "                      trainable=False))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(32, activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "print(model_2.summary())\n",
    "\n",
    "# Notice that we now have far fewer trainable parameters.\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history2 = model_2.fit(x_train, y_train,\n",
    "                       epochs=30,\n",
    "                       batch_size=128,\n",
    "                       validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting using the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our model\n",
    "# To determine the sentiment of a text, we can now use our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo on a positive text\n",
    "my_text = 'I love dogs. Dogs are the best.' \\\n",
    "          ' They are lovely, cuddly animals that only want the best for humans.'\n",
    "\n",
    "seq = reviews_tokenizer.texts_to_sequences([my_text])\n",
    "print('raw seq:', seq)\n",
    "\n",
    "seq = pad_sequences(seq, maxlen=maxlen)\n",
    "print('padded seq:', seq)\n",
    "\n",
    "prediction = model_1.predict(seq)\n",
    "print('positivity:', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict_new_comment(text: str,\n",
    "                        tokenizer: Tokenizer,\n",
    "                        seq_max_length: int,\n",
    "                        model: Sequential):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text: str. \n",
    "    :param tokenizer: The fitted keras tokenizer\n",
    "    :param seq_max_length: Max tokens to user from sequence\n",
    "    :param model: Trained keras sequential model\n",
    "    :return: int. \n",
    "    \"\"\"\n",
    "\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    print('raw seq:', seq)\n",
    "\n",
    "    seq = pad_sequences(seq,\n",
    "                        maxlen=seq_max_length)\n",
    "\n",
    "    print('padded seq:', seq)\n",
    "\n",
    "    prediction = model.predict(seq)\n",
    "\n",
    "    prob = 100 * prediction[0][0]\n",
    "\n",
    "    print('Positivity probability: {} %'.format(round(prob, 3)))\n",
    "\n",
    "    return int(prob > 50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo on a negative text\n",
    "my_text = 'The bleak economic outlook will force many small businesses into bankruptcy.'\n",
    "\n",
    "predict_new_comment(text=my_text,\n",
    "                    tokenizer=reviews_tokenizer,\n",
    "                    seq_max_length=maxlen,\n",
    "                    model=model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the mistake ? Hint (tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
