{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Content Analytics - AUEB\n",
    "\n",
    "## Callbacks that reduce training time using Keras\n",
    "\n",
    "* Lab Assistant: George Perakis\n",
    "* Email: gperakis[at]aeub.gr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# used to create mock-up data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Mock-up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.379084</td>\n",
       "      <td>-0.937865</td>\n",
       "      <td>-0.297474</td>\n",
       "      <td>0.117213</td>\n",
       "      <td>0.239456</td>\n",
       "      <td>0.894445</td>\n",
       "      <td>2.547377</td>\n",
       "      <td>1.137367</td>\n",
       "      <td>-0.224268</td>\n",
       "      <td>0.559724</td>\n",
       "      <td>-0.182770</td>\n",
       "      <td>0.487008</td>\n",
       "      <td>-0.435420</td>\n",
       "      <td>0.850456</td>\n",
       "      <td>-1.090675</td>\n",
       "      <td>-1.178499</td>\n",
       "      <td>0.933990</td>\n",
       "      <td>-0.194062</td>\n",
       "      <td>-1.012124</td>\n",
       "      <td>-0.299009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.274767</td>\n",
       "      <td>0.931978</td>\n",
       "      <td>0.190487</td>\n",
       "      <td>-0.592741</td>\n",
       "      <td>0.287354</td>\n",
       "      <td>-0.586296</td>\n",
       "      <td>0.109829</td>\n",
       "      <td>-0.911711</td>\n",
       "      <td>0.617918</td>\n",
       "      <td>-2.540849</td>\n",
       "      <td>0.290141</td>\n",
       "      <td>-2.652183</td>\n",
       "      <td>0.213764</td>\n",
       "      <td>-1.621193</td>\n",
       "      <td>-1.590486</td>\n",
       "      <td>0.108250</td>\n",
       "      <td>-0.151673</td>\n",
       "      <td>-3.966430</td>\n",
       "      <td>-2.041659</td>\n",
       "      <td>1.377930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.664970</td>\n",
       "      <td>0.866321</td>\n",
       "      <td>-0.882064</td>\n",
       "      <td>0.052033</td>\n",
       "      <td>1.323550</td>\n",
       "      <td>-0.286776</td>\n",
       "      <td>-0.463751</td>\n",
       "      <td>-0.697133</td>\n",
       "      <td>-1.573636</td>\n",
       "      <td>1.031752</td>\n",
       "      <td>0.279371</td>\n",
       "      <td>-0.963318</td>\n",
       "      <td>1.762592</td>\n",
       "      <td>0.170219</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>1.826357</td>\n",
       "      <td>-0.766580</td>\n",
       "      <td>-0.949980</td>\n",
       "      <td>-1.103628</td>\n",
       "      <td>0.329588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.084782</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.875120</td>\n",
       "      <td>1.645179</td>\n",
       "      <td>0.830084</td>\n",
       "      <td>0.394436</td>\n",
       "      <td>-0.280523</td>\n",
       "      <td>-0.626699</td>\n",
       "      <td>-1.396592</td>\n",
       "      <td>0.638423</td>\n",
       "      <td>1.953321</td>\n",
       "      <td>0.021131</td>\n",
       "      <td>-0.352626</td>\n",
       "      <td>0.241072</td>\n",
       "      <td>-0.226534</td>\n",
       "      <td>1.534620</td>\n",
       "      <td>-0.360904</td>\n",
       "      <td>-1.576495</td>\n",
       "      <td>-0.373071</td>\n",
       "      <td>-0.255314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.141583</td>\n",
       "      <td>1.279575</td>\n",
       "      <td>1.007061</td>\n",
       "      <td>-1.017441</td>\n",
       "      <td>-0.884840</td>\n",
       "      <td>1.110146</td>\n",
       "      <td>0.367618</td>\n",
       "      <td>1.389828</td>\n",
       "      <td>-1.398979</td>\n",
       "      <td>-0.921820</td>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.976645</td>\n",
       "      <td>-0.684824</td>\n",
       "      <td>-1.091520</td>\n",
       "      <td>-0.522616</td>\n",
       "      <td>1.453910</td>\n",
       "      <td>-0.914803</td>\n",
       "      <td>-0.186849</td>\n",
       "      <td>-0.449709</td>\n",
       "      <td>0.493157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.379084 -0.937865 -0.297474  0.117213  0.239456  0.894445  2.547377   \n",
       "1  0.274767  0.931978  0.190487 -0.592741  0.287354 -0.586296  0.109829   \n",
       "2 -0.664970  0.866321 -0.882064  0.052033  1.323550 -0.286776 -0.463751   \n",
       "3  0.084782  0.582900  0.875120  1.645179  0.830084  0.394436 -0.280523   \n",
       "4 -2.141583  1.279575  1.007061 -1.017441 -0.884840  1.110146  0.367618   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  1.137367 -0.224268  0.559724 -0.182770  0.487008 -0.435420  0.850456   \n",
       "1 -0.911711  0.617918 -2.540849  0.290141 -2.652183  0.213764 -1.621193   \n",
       "2 -0.697133 -1.573636  1.031752  0.279371 -0.963318  1.762592  0.170219   \n",
       "3 -0.626699 -1.396592  0.638423  1.953321  0.021131 -0.352626  0.241072   \n",
       "4  1.389828 -1.398979 -0.921820  0.010304  0.976645 -0.684824 -1.091520   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0 -1.090675 -1.178499  0.933990 -0.194062 -1.012124 -0.299009  \n",
       "1 -1.590486  0.108250 -0.151673 -3.966430 -2.041659  1.377930  \n",
       "2  0.224306  1.826357 -0.766580 -0.949980 -1.103628  0.329588  \n",
       "3 -0.226534  1.534620 -0.360904 -1.576495 -0.373071 -0.255314  \n",
       "4 -0.522616  1.453910 -0.914803 -0.186849 -0.449709  0.493157  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_feats = 20\n",
    "\n",
    "X, y = make_classification(n_samples=100_000,\n",
    "                           n_features=20,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_classes=2,\n",
    "                           n_clusters_per_class=2)\n",
    "\n",
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# we can think of this chunk as the input layer\n",
    "model.add(Dense(128, input_dim=X_train.shape[1]))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# we can think of this chunk as the hidden layer    \n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# we can think of this chunk as the output layer\n",
    "model.add(Dense(1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# setting up the optimization of our weights \n",
    "sgd = SGD(lr=0.1,\n",
    "          decay=1e-6,\n",
    "          momentum=0.9,\n",
    "          nesterov=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               2688      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,781\n",
      "Trainable params: 11,395\n",
      "Non-trainable params: 386\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_loss'\n",
    "\n",
    "model_fname = 'model.h5'\n",
    "\n",
    "callbacks = [\n",
    "        \n",
    "#     TensorBoard(log_dir=TENSORBOARD_LOGS_DIR,\n",
    "#                 histogram_freq=0,\n",
    "#                 embeddings_freq=0,\n",
    "#                 write_graph=True,\n",
    "#                 write_images=False),\n",
    "    \n",
    "    # Stop training when a monitored quantity has stopped improving.\n",
    "    EarlyStopping(monitor=monitor,\n",
    "                  patience=5,\n",
    "                  verbose=1,\n",
    "                  restore_best_weights=True),\n",
    "    \n",
    "    # Save the model after every epoch of the monitor quantity improves\n",
    "    ModelCheckpoint(filepath=model_fname,\n",
    "                    monitor=monitor,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    verbose=1),\n",
    "    \n",
    "    # Reduce learning rate when a metric has stopped improving\n",
    "    ReduceLROnPlateau(monitor=monitor,\n",
    "                      factor=0.1,\n",
    "                      patience=3,\n",
    "                      verbose=1)\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fitting the model on the data\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=500,\n",
    "                    batch_size=16,\n",
    "                    validation_split=0.2, \n",
    "                    verbose = 1,\n",
    "                    callbacks= callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras.models import load_model\n",
    "\n",
    "# model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    "\n",
    "# # returns a compiled model\n",
    "# # identical to the previous one\n",
    "# model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_keras_history(history):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param history: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # the history object gives the metrics keys. \n",
    "    # we will store the metrics keys that are from the training sesion.\n",
    "    metrics_names = [key for key in history.history.keys() if not key.startswith('val_')]\n",
    "\n",
    "    for i, metric in enumerate(metrics_names):\n",
    "        \n",
    "        # getting the training values\n",
    "        metric_train_values = history.history.get(metric, [])\n",
    "        \n",
    "        # getting the validation values\n",
    "        metric_val_values = history.history.get(\"val_{}\".format(metric), [])\n",
    "\n",
    "        # As loss always exists as a metric we use it to find the \n",
    "        epochs = range(1, len(metric_train_values) + 1)\n",
    "        \n",
    "        # leaving extra spaces to allign with the validation text\n",
    "        training_text = \"   Training {}: {:.5f}\".format(metric,\n",
    "                                                        metric_train_values[-1])\n",
    "\n",
    "        # metric\n",
    "        plt.figure(i, figsize=(12, 6))\n",
    "\n",
    "        plt.plot(epochs,\n",
    "                 metric_train_values,\n",
    "                 'b',\n",
    "                 label=training_text)\n",
    "        \n",
    "        # if we validation metric exists, then plot that as well\n",
    "        if metric_val_values:\n",
    "            validation_text = \"Validation {}: {:.5f}\".format(metric,\n",
    "                                                             metric_val_values[-1])\n",
    "\n",
    "            plt.plot(epochs,\n",
    "                     metric_val_values,\n",
    "                     'g',\n",
    "                     label=validation_text)\n",
    "        \n",
    "        # add title, xlabel, ylabe, and legend\n",
    "        plt.title('Model Metric: {}'.format(metric))\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric.title())\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra read\n",
    "# https://en.wikipedia.org/wiki/Simulated_annealing\n",
    "# You may create a CLR (cyclic learing rate) callback\n",
    "\n",
    "# https://github.com/bckenstler/CLR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
