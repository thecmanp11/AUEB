{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using IMDB movie review and we will classify reviews by sentiment\n",
    "\n",
    "need to pad sequences bc each review has diff lengths\n",
    "    so we need to provide a way that each reivwe has the same input\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "Embeddings\n",
    "    words and tokens are categorical features that CANNOT go right into a NN\n",
    "    so we ID them into an index\n",
    "        we map them into numbers. they are not more or less significant, but just a mapped number from a word or token\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    0 = negative comment\n",
    "    1 = positve comment\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "pre trained embeddinegs\n",
    "    trained using different corpora\n",
    "    try to pick ones that make sense for the text you're going to use\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    some words in your corpora won't be in the pre trainedembessing... so for these use random embeddings\n",
    "    so these words we dont have a trained vector\n",
    "    we use the same mean and std of the embeddings we DO have...\n",
    "    we create a random sample with the same mean and std dv\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Evaluation\n",
    "\n",
    "    ROC curve?\n",
    "    \n",
    "        top left is better\n",
    "        \n",
    "        \n",
    "        binary problem, use as many classifiers at once in same plot\n",
    "        but mjulti class there are too many lines so do one at a time\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    " Regularization\n",
    " \n",
    "     to address overfitting\n",
    "         we used the dropout layer\n",
    "             drop ranfom neours for each layer\n",
    "             bc feet forward nn overfit because they assign big weights\n",
    "             so we force the model to take info from all data in dataset; not just specific neurons\n",
    "             \n",
    "             so if one is really strong, we could turn it off so it tried to learn from everything else\n",
    "             \n",
    "             \n",
    "             \n",
    "             drop out lyaer only in training\n",
    "             \n",
    "             \n",
    "             AFTER activation layer\n",
    "             \n",
    "             \n",
    "     \n",
    "     \n",
    "     Batch normalization\n",
    "         just another layer\n",
    "         layer that can be used btwn linear and non linear layer in network\n",
    "         \n",
    "         for each layer, we have two steps\n",
    "             1) calc linear\n",
    "                 multies btwn data and weights\n",
    "                 linear tranny\n",
    "            2) on these outputs, pass thru activiation which is non linear\n",
    "                like relu\n",
    "                \n",
    "                \n",
    "                \n",
    "       where to put batch normalization step?\n",
    "           btwn linear and on lin active function\n",
    "           OR\n",
    "           after activation function\n",
    "               of each layer\n",
    "               \n",
    "      AI SUMMER.com / regularization\n",
    "      \n",
    "      with matrix multiplication, all the points produced (y's) are moving into another space\n",
    "      \n",
    "      clac mean and stdv\n",
    "      subtr mean and / std dv\n",
    "      \n",
    "      at each layer it goes and rescales the data\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "How to stop overfitting\n",
    "\n",
    "    keras has CALLBACKS\n",
    "        hooks in model to change each behavoir\n",
    "        \n",
    "        get data\n",
    "        split, train validation\n",
    "        build model\n",
    "        \n",
    "        \n",
    "        now\n",
    "        \n",
    "        \n",
    "        add callbacks\n",
    "        \n",
    "            to create, need to monitor validation loss (montior value)\n",
    "                could use validation accuracy, but usually it's the loss\n",
    "        \n",
    "            list\n",
    "            \n",
    "            TEnsorBoard\n",
    "                keeps in a directroy all necessary info about mateix used in model and you can use these logs to create a board which is interactive which provides necessary plots about loss accuracy any metric used\n",
    "                basically you can wathc during traiing what it does\n",
    "                \n",
    "                \n",
    "            Early Stopping\n",
    "                patience param = 5 (just an exaplme)\n",
    "                    monitor your monitor val,  at first loss is infinity\n",
    "                        after first epoch, it goes ti like 5\n",
    "                        3\n",
    "                        2\n",
    "                        etc\n",
    "                        if for 5 consequtive epochs, it doesn't improve. then the model stops trainning\n",
    "                        5 here is just an example that we put in the patience param\n",
    "               restore best weight\n",
    "                   restore best weight when you have min validation loss\n",
    "                   False, return overfitted weights at 8th epoch, but what we need to do is go back to 3rd epoch with min validloss and use this instead\n",
    "                   so we use True to do this\n",
    "                        \n",
    "                        \n",
    "          Model Checkpoint\n",
    "              uses monitor qty (validation loss (here at least))\n",
    "              save model in .h5 file as we progress\n",
    "                  save_best_model = True (obvi)\n",
    "                  \n",
    "                  False? why would we want this.\n",
    "                  it saves weights for each epoch\n",
    "                  \n",
    "                  basically you can use multiple models to decide something\n",
    "                      not JUST the best best one\n",
    "              \n",
    "              \n",
    "          \n",
    "          Reduce on platuea\n",
    "              if it cant find minimum\n",
    "              uses a factor\n",
    "                  take learning rate and decrease it\n",
    "                  \n",
    "                \n",
    "               \n",
    "            \n",
    "      \n",
    "\n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
