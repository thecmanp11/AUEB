{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"a_keras_mnist_cnn.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"hANh-XUtP50r"},"source":["# Big Data Content Analytics - AUEB\n","\n","## Introduction to Convolutional Networks for Image classification\n","\n","* Lab Assistant: George Perakis\n","* Email: gperakis[at]aeub.gr | perakisgeorgios[at]gmail.com"]},{"cell_type":"markdown","metadata":{"id":"j6n1cqNTP50u"},"source":["### Importing Modules"]},{"cell_type":"code","metadata":{"id":"TwzL0LQtP50w"},"source":["import numpy as np\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.python.keras.datasets import mnist\n","\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","from tensorflow.python.keras.losses import categorical_crossentropy\n","from tensorflow.python.keras import backend as K\n","from tensorflow.keras.optimizers import Adadelta, Adam\n","\n","from typing import Tuple, List, Dict\n","import matplotlib.pyplot as plt\n","\n","# %matplotlib notebook\n","# %pylab inline\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"-yGKkbf-P508"},"source":["### Setting experiment hyperparameters"]},{"cell_type":"code","metadata":{"id":"wHQSmWWIP50-"},"source":["# Input image dimensions\n","img_rows, img_cols = 28, 28"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hz_5DevXP51H"},"source":["### Loading MNIST dataset"]},{"cell_type":"code","metadata":{"id":"X9acLsmsP51J"},"source":["# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vr_wasYBP51S"},"source":["print(f'x_train shape: {x_train.shape}')\n","print(f'x_test shape: {x_test.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VMTj9aeP51f"},"source":["print(f'y_train shape: {y_train.shape}')\n","print(f'y_test shape: {y_test.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"code_folding":[],"id":"Z9yXQVSWP51p"},"source":["def reshape_img_input(x: np.ndarray,\n","                      img_rows: int = 28,\n","                      img_cols: int = 28,\n","                      normalize: bool = True) -> Tuple:\n","    \"\"\"\n","    This function reshapes a n-dimensional numpy array of images into another format.\n","    Also normalizes the images\n","    \n","    :param x: N-dimensional array containing images\n","    :param img_rows: The output width of each of the images\n","    :param img_cols: The output height of each of the images\n","    :param normalize: Whether to normalize the images or not\n","    :return: A numpy array with the transformed images and the shape of each observation (image)\n","    \"\"\"\n","    \n","    print(f'Original shape of nd-array: {x.shape}')\n","    \n","    if K.image_data_format() == 'channels_first':\n","        \n","        # the channel dimension goes to the front\n","        x1 = x.reshape(x.shape[0], 1, img_rows, img_cols)\n","\n","        input_shape = (1, img_rows, img_cols)\n","\n","    else:\n","        # the channel dimension goes to the end. So we end up with the following 4-D tensor\n","        # (N-samples, Height, Width, N-channels)\n","        x1 = x.reshape(x.shape[0], img_rows, img_cols, 1)\n","\n","        input_shape = (img_rows, img_cols, 1)\n","\n","    x1 = x1.astype('float32')\n","\n","    if normalize:\n","        x1 /= 255\n","    \n","    \n","    return x1, input_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AAlbGQQP512"},"source":["x_train, input_shape = reshape_img_input(x=x_train,\n","                                         img_rows=28, \n","                                         img_cols=28,\n","                                         normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpm3a0KcP52C"},"source":["x_test, _ = reshape_img_input(x=x_test,\n","                              img_rows=28, \n","                              img_cols=28,\n","                              normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERzp2soOP52K"},"source":["print(f'Input Shape: {input_shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWlGxDC5P52T"},"source":["print(f'x_train shape: {x_train.shape}')\n","\n","print(f'{x_train.shape[0]} train samples')\n","print(f'{x_test.shape[0]} test samples')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DB1vyivVP52b"},"source":["### Conversion of the Y labels to One-Hot-Encoding"]},{"cell_type":"code","metadata":{"id":"BreO53uLP52e"},"source":["# pre-proccessing parameters\n","num_classes = 10\n","\n","# convert class vectors to binary class matrices\n","y_train = to_categorical(y_train, num_classes)\n","\n","y_test = to_categorical(y_test, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aquMY_p2P52p"},"source":["# checking the difference in the shape\n","print('y_train shape: {}'.format(y_train.shape))\n","print('y_test shape: {}'.format(y_test.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_vhqyVLP52x"},"source":["y_train.sum(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9nVi3n6P525"},"source":["# summing for each column (axis=0) to check the number of occurences of each digit.\n","# dividing with the total sum, in order to find the ratios\n","\n","ratios = y_train.sum(axis=0) / y_train.sum()\n","\n","for digit, value in enumerate(ratios):\n","    print(\"Digit: {} | Label Ratio: {:.2f} %\".format(digit, 100 * value))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"code_folding":[],"id":"0F740sGBP53E"},"source":["def plot_digits_examples(x, y):\n","    \"\"\"\n","    This function plots examples of digits (the first 9)\n","    \n","    :param x: the nd-array of the images\n","    :param y: the labels of the images\n","    :return: None\n","    \"\"\"\n","    \n","    # creating a figure for all the digits that will be plotted\n","    fig1 = plt.figure()\n","    \n","    print(list(range(10)))\n","    print('-' * 31)\n","    \n","    for i in range(9):\n","        # create a subplot\n","        ax = fig1.add_subplot(191 + i)\n","        ax.clear()\n","        # plot the actual digit\n","        ax.imshow(x[i].reshape(28, 28), cmap='gray')\n","        \n","        print(y[i])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_SNn9eDP53M"},"source":["plot_digits_examples(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EXiA8AxJP53V","scrolled":true},"source":["plot_digits_examples(x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVzqLXuDP53g"},"source":["## How Convolutions Work"]},{"cell_type":"markdown","metadata":{"id":"TCAqlaMTP53i"},"source":["<img src=\"https://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif\">"]},{"cell_type":"markdown","metadata":{"id":"ZbyUsq9xP53k"},"source":["## What are Strides and Padding"]},{"cell_type":"markdown","metadata":{"id":"Rrqg7UwyP53m"},"source":["<img src=\"https://theano-pymc.readthedocs.io/en/latest/_images/numerical_padding_strides.gif\">"]},{"cell_type":"markdown","metadata":{"id":"FiO6Ecz2P53o"},"source":["## How Max Pooling Works"]},{"cell_type":"markdown","metadata":{"id":"UQ-XOXpwP53q"},"source":["<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\">"]},{"cell_type":"markdown","metadata":{"id":"0rU5BBOcP53r"},"source":["## Build CNN Model"]},{"cell_type":"markdown","metadata":{"id":"z9drNdh2P53t"},"source":["### Set hyperparameters"]},{"cell_type":"code","metadata":{"id":"0drMzWAGP53u"},"source":["from tensorflow.python.keras.callbacks import EarlyStopping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNnCf6EJP532"},"source":["# Setting model hyperparameters\n","batch_size = 1024\n","\n","epochs = 500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-lV5TxcP53-"},"source":["### Create Model Structure"]},{"cell_type":"code","metadata":{"id":"fkuqxYYKP54A"},"source":["# Creating a sequential model\n","model = Sequential()\n","\n","# adding a 2D Convolutional layer with 32 neurons with kernel size of 3X3, with relu activation.\n","model.add(Conv2D(32,\n","                 kernel_size=(3, 3), \n","                 activation='relu',\n","                 input_shape=input_shape))\n","\n","# adding a 2D Convolutional layer with 64 neurons with kernel size of 3X3, with relu activation.\n","model.add(Conv2D(64, \n","                 kernel_size=(3, 3), \n","                 activation='relu'))\n","\n","# Adding a Max Pooling layer to cut the size of the conv-layers \n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Adding dropout to regularize the model\n","model.add(Dropout(0.25))\n","\n","# Flatten the MaxPooling Layer.\n","model.add(Flatten())\n","\n","# Adding another Dense layer of 128 neurons with Relu activation.\n","model.add(Dense(128, activation='relu'))\n","\n","# Adding dropout to regularize the model\n","model.add(Dropout(0.4))\n","\n","# We have a MULTI-CLASS problem. This is the reason we use 10 neurons (as many as the digits)\n","# with the SOFTMAX activation. \n","model.add(Dense(num_classes,\n","                activation='softmax'))\n","\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3RR1e--P54L"},"source":["# How to plot a nice neural net represenation using LaTeX\n","# https://github.com/HarisIqbal88/PlotNeuralNet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GbNTZwQEP54T"},"source":["### Model compilation"]},{"cell_type":"code","metadata":{"id":"lqTPvBySP54U"},"source":["# Remember, we have a multi-class problem to solve with 10 classes\n","# that's why we will use the categorical_crossentropy as a loss function. \n","\n","# Also, introducing another variation of the optimizers, the Adadelta().\n","# You could also use the Adam as well. \n","model.compile(\n","    loss=categorical_crossentropy,\n","    optimizer=Adadelta(),\n","    metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qm2zEo6mP54c"},"source":["### Setting Callbacks"]},{"cell_type":"code","metadata":{"id":"z76_pUA8P54f"},"source":["# help(keras.callbacks.EarlyStopping)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkND9590P54x"},"source":["# early stopping callback\n","\n","es = EarlyStopping(\n","    monitor   = 'val_loss', # which metric we want to use as criterion to stop training\n","    min_delta = 0, # Minimum change in the monitored quantity to qualify as an improvement\n","    patience  = 3, # we 3 epochs before stopping\n","    verbose   = 1, # verbosity level\n","    mode      = 'auto',\n","    restore_best_weights = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JO5qA09KP543"},"source":["### Fitting the Model"]},{"cell_type":"code","metadata":{"id":"Iop1EILwP544","scrolled":false},"source":["model.fit(\n","    x_train,\n","    y_train,\n","    batch_size = batch_size,\n","    epochs = epochs,\n","    verbose = 1,\n","    validation_split = 0.1,\n","    callbacks = [es])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KFk6788KP54_"},"source":["### Model Evaluation"]},{"cell_type":"code","metadata":{"id":"_KTWo1LrP55B","scrolled":true},"source":["score = model.evaluate(x_test,\n","                       y_test,\n","                       verbose=1)\n","\n","print(f'Test loss: {score[0]}:')\n","print('Test accuracy: {:.3f} %'.format(100 * score[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-ej3qF_P55J"},"source":["y_test_pred = model.predict_classes(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8PnO0WbW3Jd"},"source":["y_test_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VW6qIr75P55j"},"source":["y_test.argmax(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0KLwJWSqP55s"},"source":["from sklearn.metrics import confusion_matrix, classification_report\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ojUffxXP55y"},"source":["conf_mat = confusion_matrix(y_test.argmax(axis=1),\n","                            y_test_pred)\n","\n","pd.DataFrame(conf_mat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KF7_9x-bP555"},"source":["print(classification_report(y_test.argmax(axis=1),\n","                            y_test_pred,\n","                            digits=4))"],"execution_count":null,"outputs":[]}]}