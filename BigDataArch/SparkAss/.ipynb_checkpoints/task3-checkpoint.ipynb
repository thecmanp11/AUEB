{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.1.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile \n",
    "pd.set_option(\"display.max_columns\",1000)\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "#findspark.init('C:\\Spark\\spark-3.1.1-bin-hadoop2.7')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark df\n",
    "In order to work with spark within python, we need to create a Spark Session. Here we call the app ' Project1 ' and we assign this to a python object named 'spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Project1').getOrCreate()#.master(\"local\") #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV to python df\n",
    "We pull in the flights.csv first as a pandas dataframe in python. Within python, we can do some basic cleaning and manipulation for the tasks ahead.\n",
    "\n",
    "*important*: \n",
    "\n",
    "Here, in preparation for task 3, we create a column named 'DEP_HR' which is a string containing just the hour that the flight departed. We will use this as our y variable in our regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY_NAME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_CODE</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>DEP_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N8974C</td>\n",
       "      <td>9E</td>\n",
       "      <td>AVL</td>\n",
       "      <td>Asheville, NC</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1658</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1758</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N922XJ</td>\n",
       "      <td>9E</td>\n",
       "      <td>JFK</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>RDU</td>\n",
       "      <td>Raleigh/Durham, NC</td>\n",
       "      <td>1122</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1255</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N326PQ</td>\n",
       "      <td>9E</td>\n",
       "      <td>CLE</td>\n",
       "      <td>Cleveland, OH</td>\n",
       "      <td>DTW</td>\n",
       "      <td>Detroit, MI</td>\n",
       "      <td>1334</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1417</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N135EV</td>\n",
       "      <td>9E</td>\n",
       "      <td>BHM</td>\n",
       "      <td>Birmingham, AL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1255</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N914XJ</td>\n",
       "      <td>9E</td>\n",
       "      <td>GTF</td>\n",
       "      <td>Great Falls, MT</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>1057</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1418</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N257NN</td>\n",
       "      <td>MQ</td>\n",
       "      <td>STL</td>\n",
       "      <td>St. Louis, MO</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>1220</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1327</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N855AE</td>\n",
       "      <td>MQ</td>\n",
       "      <td>LGA</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>CMH</td>\n",
       "      <td>Columbus, OH</td>\n",
       "      <td>1048</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>1233</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N688AE</td>\n",
       "      <td>MQ</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>COU</td>\n",
       "      <td>Columbia, MO</td>\n",
       "      <td>2317</td>\n",
       "      <td>52.0</td>\n",
       "      <td>104</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N262NN</td>\n",
       "      <td>MQ</td>\n",
       "      <td>MSN</td>\n",
       "      <td>Madison, WI</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N228NN</td>\n",
       "      <td>MQ</td>\n",
       "      <td>DFW</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>LEX</td>\n",
       "      <td>Lexington, KY</td>\n",
       "      <td>1821</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2120</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FL_DATE TAIL_NUM CARRIER ORIGIN       ORIGIN_CITY_NAME DEST  \\\n",
       "0    2019-01-01   N8974C      9E    AVL          Asheville, NC  ATL   \n",
       "1    2019-01-01   N922XJ      9E    JFK           New York, NY  RDU   \n",
       "2    2019-01-01   N326PQ      9E    CLE          Cleveland, OH  DTW   \n",
       "3    2019-01-01   N135EV      9E    BHM         Birmingham, AL  ATL   \n",
       "4    2019-01-01   N914XJ      9E    GTF        Great Falls, MT  MSP   \n",
       "..          ...      ...     ...    ...                    ...  ...   \n",
       "995  2019-01-01   N257NN      MQ    STL          St. Louis, MO  ORD   \n",
       "996  2019-01-01   N855AE      MQ    LGA           New York, NY  CMH   \n",
       "997  2019-01-01   N688AE      MQ    ORD            Chicago, IL  COU   \n",
       "998  2019-01-01   N262NN      MQ    MSN            Madison, WI  ORD   \n",
       "999  2019-01-01   N228NN      MQ    DFW  Dallas/Fort Worth, TX  LEX   \n",
       "\n",
       "         DEST_CITY_NAME DEP_TIME DEP_DELAY ARR_TIME ARR_DELAY CANCELLED  \\\n",
       "0           Atlanta, GA     1658      -7.0     1758     -22.0       0.0   \n",
       "1    Raleigh/Durham, NC     1122      -8.0     1255     -29.0       0.0   \n",
       "2           Detroit, MI     1334      -7.0     1417     -31.0       0.0   \n",
       "3           Atlanta, GA     1059      -1.0     1255      -8.0       0.0   \n",
       "4       Minneapolis, MN     1057      -3.0     1418     -17.0       0.0   \n",
       "..                  ...      ...       ...      ...       ...       ...   \n",
       "995         Chicago, IL     1220      14.0     1327      -7.0       0.0   \n",
       "996        Columbus, OH     1048     -12.0     1233     -34.0       0.0   \n",
       "997        Columbia, MO     2317      52.0      104      80.0       0.0   \n",
       "998         Chicago, IL        0       0.0        0       0.0       1.0   \n",
       "999       Lexington, KY     1821      36.0     2120      32.0       0.0   \n",
       "\n",
       "    CANCELLATION_CODE DIVERTED CARRIER_DELAY WEATHER_DELAY NAS_DELAY  \\\n",
       "0                   0      0.0           0.0           0.0       0.0   \n",
       "1                   0      0.0           0.0           0.0       0.0   \n",
       "2                   0      0.0           0.0           0.0       0.0   \n",
       "3                   0      0.0           0.0           0.0       0.0   \n",
       "4                   0      0.0           0.0           0.0       0.0   \n",
       "..                ...      ...           ...           ...       ...   \n",
       "995                 0      0.0           0.0           0.0       0.0   \n",
       "996                 0      0.0           0.0           0.0       0.0   \n",
       "997                 0      0.0           0.0           0.0      28.0   \n",
       "998                 B      0.0           0.0           0.0       0.0   \n",
       "999                 0      0.0          32.0           0.0       0.0   \n",
       "\n",
       "    SECURITY_DELAY LATE_AIRCRAFT_DELAY Unnamed: 19 DEP_HR  \n",
       "0              0.0                 0.0         0.0     16  \n",
       "1              0.0                 0.0         0.0     11  \n",
       "2              0.0                 0.0         0.0     13  \n",
       "3              0.0                 0.0         0.0     10  \n",
       "4              0.0                 0.0         0.0     10  \n",
       "..             ...                 ...         ...    ...  \n",
       "995            0.0                 0.0         0.0     12  \n",
       "996            0.0                 0.0         0.0     10  \n",
       "997            0.0                52.0         0.0     23  \n",
       "998            0.0                 0.0         0.0         \n",
       "999            0.0                 0.0         0.0     18  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('flights.csv.zip')\n",
    "df = df.fillna(0)\n",
    "df['DEP_TIME'] = df['DEP_TIME'].astype('int')\n",
    "df['DEP_HR'] = df['DEP_TIME'].astype('str').str[:-2] \n",
    "df['ARR_TIME'] = df['ARR_TIME'].astype('int')\n",
    "df = df.fillna(0)\n",
    "df = df.astype('str')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Schema\n",
    "for ease of loading into Spark, we make everything a string initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, DateType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FL_DATE\", StringType()),#, DateType()),\n",
    "    StructField(\"TAIL_NUM\", StringType()),\n",
    "    StructField(\"CARRIER\", StringType()),\n",
    "    StructField(\"ORIGIN\", StringType()),\n",
    "    StructField(\"ORIGIN_CITY_NAME\", StringType()),\n",
    "    StructField(\"DEST\", StringType()),\n",
    "    StructField(\"DEST_CITY_NAME\", StringType()),\n",
    "    StructField(\"DEP_TIME\", StringType()),\n",
    "    StructField(\"DEP_DELAY\", StringType()),#, DoubleType()),\n",
    "    StructField(\"ARR_TIME\", StringType()),\n",
    "    StructField(\"ARR_DELAY\", StringType()),#, DoubleType()),\n",
    "    StructField(\"CANCELLED\", StringType()),\n",
    "    StructField(\"CANCELLATION_CODE\", StringType()),\n",
    "    StructField(\"DIVERTED\", StringType()),\n",
    "    StructField(\"CARRIER_DELAY\", StringType()),\n",
    "    StructField(\"WEATHER_DELAY\", StringType()),\n",
    "    StructField(\"NAS_DELAY\", StringType()),\n",
    "    StructField(\"SECURITY_DELAY\", StringType()),\n",
    "    StructField(\"LATE_AIRCRAFT_DELAY\", StringType()),\n",
    "    StructField(\"Unnamed: 19\", StringType()),#, IntegerType())\n",
    "    StructField(\"DEP_HR\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark DF from pandas df\n",
    "Now we create a Spark dataframe from our pandas dataframe in python. We print the schema just to review it and verify that everything worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- TAIL_NUM: string (nullable = true)\n",
      " |-- CARRIER: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY: string (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CARRIER_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      " |-- NAS_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- Unnamed: 19: string (nullable = true)\n",
      " |-- DEP_HR: string (nullable = true)\n",
      "\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|    ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 19|DEP_HR|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|       Asheville, NC| ATL|       Atlanta, GA|    1658|     -7.0|    1758|    -22.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    16|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|        New York, NY| RDU|Raleigh/Durham, NC|    1122|     -8.0|    1255|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    11|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|       Cleveland, OH| DTW|       Detroit, MI|    1334|     -7.0|    1417|    -31.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    13|\n",
      "|2019-01-01|  N135EV|     9E|   BHM|      Birmingham, AL| ATL|       Atlanta, GA|    1059|     -1.0|    1255|     -8.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    10|\n",
      "|2019-01-01|  N914XJ|     9E|   GTF|     Great Falls, MT| MSP|   Minneapolis, MN|    1057|     -3.0|    1418|    -17.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    10|\n",
      "|2019-01-01|  N924EV|     9E|   GRB|       Green Bay, WI| DTW|       Detroit, MI|     855|      0.0|    1125|     10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N195PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|     800|     -5.0|     900|    -16.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N319PQ|     9E|   CLT|       Charlotte, NC| JFK|      New York, NY|    1350|    -10.0|    1534|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    13|\n",
      "|2019-01-01|  N933XJ|     9E|   MEM|         Memphis, TN| MSP|   Minneapolis, MN|    1441|     -4.0|    1641|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    14|\n",
      "|2019-01-01|  N933XJ|     9E|   MSP|     Minneapolis, MN| MEM|       Memphis, TN|     847|     -4.0|    1114|     -6.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N8886A|     9E|   ATL|         Atlanta, GA| AEX|    Alexandria, LA|    1856|     -5.0|    1931|    -20.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    18|\n",
      "|2019-01-01|  N302PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|    1427|     -9.0|    1540|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    14|\n",
      "|2019-01-01|  N302PQ|     9E|   ATL|         Atlanta, GA| AGS|       Augusta, GA|    1259|     -6.0|    1343|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    12|\n",
      "|2019-01-01|  N272PQ|     9E|   CVG|      Cincinnati, OH| LGA|      New York, NY|     834|     -6.0|    1022|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N292PQ|     9E|   RDU|  Raleigh/Durham, NC| MCO|       Orlando, FL|    1324|     -1.0|    1504|    -14.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    13|\n",
      "|2019-01-01|  N980EV|     9E|   GSO|Greensboro/High P...| LGA|      New York, NY|    1155|     -5.0|    1340|      3.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    11|\n",
      "|2019-01-01|  N326PQ|     9E|   BIS| Bismarck/Mandan, ND| MSP|   Minneapolis, MN|     809|    124.0|     933|    109.0|      0.0|                0|     0.0|        109.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N135EV|     9E|   ATL|         Atlanta, GA| AVL|     Asheville, NC|    1531|     -4.0|    1625|    -10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    15|\n",
      "|2019-01-01|  N8976E|     9E|   LGA|        New York, NY| BTV|    Burlington, VT|    1756|     -4.0|    1913|     -7.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    17|\n",
      "|2019-01-01|  N925XJ|     9E|   PIT|      Pittsburgh, PA| JFK|      New York, NY|    1124|     -1.0|    1249|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    11|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "flts=spark.createDataFrame(df,schema=schema) \n",
    "flts.printSchema()\n",
    "flts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Delays from String to Doubles\n",
    "Finally, now that we have a Spark dataframe - we know that some variables need to be transformed into numerical values. So we convert the strings DEP_DELAY, ARR_DELAY and DEP_HR into Doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|    ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 19|DEP_HR|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|       Asheville, NC| ATL|       Atlanta, GA|    1658|     -7.0|    1758|    -22.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  16.0|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|        New York, NY| RDU|Raleigh/Durham, NC|    1122|     -8.0|    1255|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  11.0|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|       Cleveland, OH| DTW|       Detroit, MI|    1334|     -7.0|    1417|    -31.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  13.0|\n",
      "|2019-01-01|  N135EV|     9E|   BHM|      Birmingham, AL| ATL|       Atlanta, GA|    1059|     -1.0|    1255|     -8.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  10.0|\n",
      "|2019-01-01|  N914XJ|     9E|   GTF|     Great Falls, MT| MSP|   Minneapolis, MN|    1057|     -3.0|    1418|    -17.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  10.0|\n",
      "|2019-01-01|  N924EV|     9E|   GRB|       Green Bay, WI| DTW|       Detroit, MI|     855|      0.0|    1125|     10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N195PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|     800|     -5.0|     900|    -16.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N319PQ|     9E|   CLT|       Charlotte, NC| JFK|      New York, NY|    1350|    -10.0|    1534|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  13.0|\n",
      "|2019-01-01|  N933XJ|     9E|   MEM|         Memphis, TN| MSP|   Minneapolis, MN|    1441|     -4.0|    1641|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  14.0|\n",
      "|2019-01-01|  N933XJ|     9E|   MSP|     Minneapolis, MN| MEM|       Memphis, TN|     847|     -4.0|    1114|     -6.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N8886A|     9E|   ATL|         Atlanta, GA| AEX|    Alexandria, LA|    1856|     -5.0|    1931|    -20.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  18.0|\n",
      "|2019-01-01|  N302PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|    1427|     -9.0|    1540|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  14.0|\n",
      "|2019-01-01|  N302PQ|     9E|   ATL|         Atlanta, GA| AGS|       Augusta, GA|    1259|     -6.0|    1343|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  12.0|\n",
      "|2019-01-01|  N272PQ|     9E|   CVG|      Cincinnati, OH| LGA|      New York, NY|     834|     -6.0|    1022|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N292PQ|     9E|   RDU|  Raleigh/Durham, NC| MCO|       Orlando, FL|    1324|     -1.0|    1504|    -14.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  13.0|\n",
      "|2019-01-01|  N980EV|     9E|   GSO|Greensboro/High P...| LGA|      New York, NY|    1155|     -5.0|    1340|      3.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  11.0|\n",
      "|2019-01-01|  N326PQ|     9E|   BIS| Bismarck/Mandan, ND| MSP|   Minneapolis, MN|     809|    124.0|     933|    109.0|      0.0|                0|     0.0|        109.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N135EV|     9E|   ATL|         Atlanta, GA| AVL|     Asheville, NC|    1531|     -4.0|    1625|    -10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  15.0|\n",
      "|2019-01-01|  N8976E|     9E|   LGA|        New York, NY| BTV|    Burlington, VT|    1756|     -4.0|    1913|     -7.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  17.0|\n",
      "|2019-01-01|  N925XJ|     9E|   PIT|      Pittsburgh, PA| JFK|      New York, NY|    1124|     -1.0|    1249|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  11.0|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "StructType(List(StructField(FL_DATE,StringType,true),StructField(TAIL_NUM,StringType,true),StructField(CARRIER,StringType,true),StructField(ORIGIN,StringType,true),StructField(ORIGIN_CITY_NAME,StringType,true),StructField(DEST,StringType,true),StructField(DEST_CITY_NAME,StringType,true),StructField(DEP_TIME,StringType,true),StructField(DEP_DELAY,DoubleType,true),StructField(ARR_TIME,StringType,true),StructField(ARR_DELAY,DoubleType,true),StructField(CANCELLED,StringType,true),StructField(CANCELLATION_CODE,StringType,true),StructField(DIVERTED,StringType,true),StructField(CARRIER_DELAY,StringType,true),StructField(WEATHER_DELAY,StringType,true),StructField(NAS_DELAY,StringType,true),StructField(SECURITY_DELAY,StringType,true),StructField(LATE_AIRCRAFT_DELAY,StringType,true),StructField(Unnamed: 19,StringType,true),StructField(DEP_HR,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "flts = flts.withColumn(\"DEP_DELAY\", flts['DEP_DELAY'].cast(DoubleType())).withColumn('ARR_DELAY', flts['ARR_DELAY'].cast(DoubleType())).withColumn(\"DEP_HR\", flts['DEP_HR'].cast(DoubleType()))\n",
    "flts.show()\n",
    "print(flts.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as the final step for the data preparation, we create a temporary SQL table named \"flights\". We will use this in the tasks to more easily manipulate the data in a \"SQL-esque\" environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flts.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 [45 points]\n",
    "As a final task, your supervisor assigned to you to investigate if it is possible to train a linear\n",
    "regression model that could predict the departure delay a flight may have by using, \n",
    "as input:\n",
    "- its origin (column “ORIGIN”)\n",
    "- its airways (column “CARRIER”)\n",
    "- its departure time (column “DEP_TIME”)\n",
    "\n",
    "Again you should use Python and DataFrames, this time with **MLlib**\n",
    "\n",
    "You should pay attention to transform the string-based input features using the proper representation\n",
    "format, and you should explain your choices.\n",
    "\n",
    "Special attention should be given to the “DEP_TIME” column: \n",
    "your supervisor told you that you should only consider the corresponding hour of the day \n",
    "- (which, of course, should be transformed in a one-hot representation).\n",
    "\n",
    "For your training and testing workflow you should first remove outliers (see Task 2) \n",
    "and then split your dataset into two parts:\n",
    "- one that will be used for training reasons and it will contain 70% of the entries\n",
    "- a second one containing the remaining entries andwhich will be used for the assessment of the model. \n",
    "\n",
    "No need to implement a more sophisticated evaluation process (e.g., based on k-fold) is required in this phase. \n",
    "\n",
    "Your code should:\n",
    "- (a) prepare the feature vectors\n",
    "- (b) prepare the training and testing datasets\n",
    "- (c) train the model\n",
    "- (d) print on the screen the first 10 predictions, i.e., pairs of feature vectors (in compact format) and predicted outputs, on the screen \n",
    "- (e) evaluate the accuracy of the model and display the corresponding metric on the screen. \n",
    "\n",
    "Your deliverables are the following:\n",
    "- A Python file (named “task3.py”) containing the code of the preprocessing, training, and evaluation phase of your machine learning workflow.\n",
    "- A report (named “task3.pdf”) explaining the basic intuition of your code and your design decisions and assumptions.\n",
    "- A screenshot (named “task3.png”) showing the output of your machine learning workflow (e.g., showing the results in the console). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers\n",
    "here we find both the top and bottom 1 percentiles and use those values to refine the dataset we will use in our ML regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15  ,  110\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    ORIGIN\n",
    "    , CARRIER\n",
    "    , DEP_HR\n",
    "    , DEP_DELAY\n",
    "    FROM flights \n",
    "    order by DEP_DELAY desc;\n",
    "    \"\"\")\n",
    "perc1 = query.stat.approxQuantile(\"DEP_DELAY\",[0.01],0.0)\n",
    "perc1 = int(perc1[0])\n",
    "perc99 = query.stat.approxQuantile(\"DEP_DELAY\",[0.99],0.0)\n",
    "perc99 = int(perc99[0])\n",
    "print(perc1,' , ',perc99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|ORIGIN|CARRIER|DEP_HR|DEP_DELAY|\n",
      "+------+-------+------+---------+\n",
      "|   TUS|     AA|  21.0|    110.0|\n",
      "|   DFW|     AA|  23.0|    101.0|\n",
      "|   GTF|     G4|  20.0|    101.0|\n",
      "|   BLI|     G4|  18.0|    100.0|\n",
      "|   CLT|     AA|  11.0|    100.0|\n",
      "|   MCI|     AA|   7.0|    100.0|\n",
      "|   PHX|     AA|  11.0|     96.0|\n",
      "|   DFW|     AA|  14.0|     95.0|\n",
      "|   AZA|     G4|  17.0|     94.0|\n",
      "|   PHL|     AA|  null|     93.0|\n",
      "|   CLT|     AA|  13.0|     83.0|\n",
      "|   PHX|     AA|  22.0|     82.0|\n",
      "|   TPA|     AA|  15.0|     81.0|\n",
      "|   ORD|     AA|  21.0|     80.0|\n",
      "|   CLT|     AA|  15.0|     79.0|\n",
      "|   DFW|     AA|  11.0|     77.0|\n",
      "|   RSW|     AA|  18.0|     71.0|\n",
      "|   MIA|     AA|  18.0|     70.0|\n",
      "|   MSN|     MQ|  19.0|     68.0|\n",
      "|   ELP|     MQ|  17.0|     67.0|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    ORIGIN\n",
    "    , CARRIER\n",
    "    , DEP_HR\n",
    "    , DEP_DELAY\n",
    "    FROM flights\n",
    "    where DEP_DELAY between {} and {}\n",
    "    order by DEP_DELAY desc;\n",
    "    \"\"\".format(perc1,perc99))\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Feature Vectors\n",
    "In the dataset, if we find any null values, we want to remove them. Another option would be to place a 0 here, but in this scenario that would lead to the creation of false data and would confuse our model.\n",
    "\n",
    "Below, we can use the DEP_HR variable itself as it's numeric, but we need to One Hot Encode both ORIGIN and CARRIER as they are strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep = query.na.drop()\n",
    "#prep.select(\"DEP_HR\").distinct().orderBy(\"DEP_HR\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "ORGindexer = StringIndexer()\\\n",
    ".setInputCol(\"ORIGIN\")\\\n",
    ".setOutputCol(\"ORIGIN_indx\")\n",
    "\n",
    "CARindexer = StringIndexer()\\\n",
    ".setInputCol(\"CARRIER\")\\\n",
    ".setOutputCol(\"CARRIER_indx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "ORGencoder = OneHotEncoder(dropLast=False)\\\n",
    ".setInputCols([\"ORIGIN_indx\"])\\\n",
    ".setOutputCols([\"ORIGIN_encd\"])\n",
    "\n",
    "CARencoder = OneHotEncoder(dropLast=False)\\\n",
    ".setInputCols([\"CARRIER_indx\"])\\\n",
    ".setOutputCols([\"CARRIER_encd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize\n",
    "we need to place all of these One Hot Encoded options into a single vector so the ML model can easily read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vector_assembler = VectorAssembler()\\\n",
    ".setInputCols([\"ORIGIN_encd\",\"CARRIER_encd\",\"DEP_HR\"])\\\n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Pipeline\n",
    "we tell Spark in which order we need it to make these transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "transformation_pipeline = Pipeline()\\\n",
    ".setStages([ORGindexer, CARindexer, ORGencoder, CARencoder, vector_assembler])\n",
    "fitted_pipeline = transformation_pipeline.fit(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression dataset\n",
    "here we have all the data we need and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+-----------+------------+----------------+-------------+--------------------+\n",
      "|ORIGIN|CARRIER|DEP_HR|DEP_DELAY|ORIGIN_indx|CARRIER_indx|     ORIGIN_encd| CARRIER_encd|            features|\n",
      "+------+-------+------+---------+-----------+------------+----------------+-------------+--------------------+\n",
      "|   TUS|     AA|  21.0|    110.0|       60.0|         0.0|(144,[60],[1.0])|(5,[0],[1.0])|(150,[60,144,149]...|\n",
      "|   GTF|     G4|  20.0|    101.0|       71.0|         3.0|(144,[71],[1.0])|(5,[3],[1.0])|(150,[71,147,149]...|\n",
      "|   DFW|     AA|  23.0|    101.0|        0.0|         0.0| (144,[0],[1.0])|(5,[0],[1.0])|(150,[0,144,149],...|\n",
      "|   BLI|     G4|  18.0|    100.0|       55.0|         3.0|(144,[55],[1.0])|(5,[3],[1.0])|(150,[55,147,149]...|\n",
      "|   CLT|     AA|  11.0|    100.0|        2.0|         0.0| (144,[2],[1.0])|(5,[0],[1.0])|(150,[2,144,149],...|\n",
      "+------+-------+------+---------+-----------+------------+----------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regr = fitted_pipeline.transform(prep)\n",
    "regr.cache()\n",
    "regr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Temp table\n",
    "again, we will place this data into a temporary SQL table so we can manipulate the dataset more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.createOrReplaceTempView(\"regr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing the dataset\n",
    "before we begin the machine learning process, we need a dataset which has only the vectorized (and encoded) features and the variable we want to predict which we name here 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(150,[60,144,149]...|110.0|\n",
      "|(150,[71,147,149]...|101.0|\n",
      "|(150,[0,144,149],...|101.0|\n",
      "|(150,[55,147,149]...|100.0|\n",
      "|(150,[2,144,149],...|100.0|\n",
      "|(150,[36,144,149]...|100.0|\n",
      "|(150,[1,144,149],...| 96.0|\n",
      "|(150,[0,144,149],...| 95.0|\n",
      "|(150,[39,147,149]...| 94.0|\n",
      "|(150,[2,144,149],...| 83.0|\n",
      "|(150,[1,144,149],...| 82.0|\n",
      "|(150,[20,144,149]...| 81.0|\n",
      "|(150,[3,144,149],...| 80.0|\n",
      "|(150,[2,144,149],...| 79.0|\n",
      "|(150,[0,144,149],...| 77.0|\n",
      "|(150,[18,144,149]...| 71.0|\n",
      "|(150,[5,144,149],...| 70.0|\n",
      "|(150,[126,148,149...| 68.0|\n",
      "|(150,[67,148,149]...| 67.0|\n",
      "|(150,[2,144,149],...| 66.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rquery = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    features\n",
    "    , DEP_DELAY as label\n",
    "    FROM regr\n",
    "    group by features\n",
    "    , DEP_DELAY;\n",
    "    \"\"\")\n",
    "rquery.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "We will use a 70% 30% split as specified in the instructions. We will use a method called randomSplit and assign each of the chunks to variables named train and test respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(150,[2,144,149],...|100.0|\n",
      "|(150,[55,147,149]...|100.0|\n",
      "|(150,[60,144,149]...|110.0|\n",
      "|(150,[71,147,149]...|101.0|\n",
      "|(150,[0,144,149],...| 95.0|\n",
      "|(150,[1,144,149],...| 96.0|\n",
      "|(150,[2,144,149],...| 83.0|\n",
      "|(150,[39,147,149]...| 94.0|\n",
      "|(150,[0,144,149],...| 77.0|\n",
      "|(150,[1,144,149],...| 82.0|\n",
      "|(150,[20,144,149]...| 81.0|\n",
      "|(150,[2,144,149],...| 66.0|\n",
      "|(150,[18,144,149]...| 71.0|\n",
      "|(150,[67,148,149]...| 67.0|\n",
      "|(150,[126,148,149...| 68.0|\n",
      "|(150,[3,144,149],...| 65.0|\n",
      "|(150,[4,144,149],...| 60.0|\n",
      "|(150,[16,144,149]...| 58.0|\n",
      "|(150,[93,144,149]...| 58.0|\n",
      "|(150,[96,147,149]...| 59.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split = rquery.randomSplit([0.7, 0.3])\n",
    "train = split[0]\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(150,[0,144,149],...|101.0|\n",
      "|(150,[36,144,149]...|100.0|\n",
      "|(150,[2,144,149],...| 79.0|\n",
      "|(150,[3,144,149],...| 80.0|\n",
      "|(150,[5,144,149],...| 70.0|\n",
      "|(150,[10,146,149]...| 59.0|\n",
      "|(150,[0,144,149],...| 57.0|\n",
      "|(150,[3,148,149],...| 52.0|\n",
      "|(150,[22,144,149]...| 50.0|\n",
      "|(150,[24,147,149]...| 49.0|\n",
      "|(150,[58,144,149]...| 49.0|\n",
      "|(150,[3,144,149],...| 48.0|\n",
      "|(150,[5,144,149],...| 47.0|\n",
      "|(150,[77,147,149]...| 47.0|\n",
      "|(150,[1,144,149],...| 44.0|\n",
      "|(150,[39,147,149]...| 45.0|\n",
      "|(150,[40,144,149]...| 44.0|\n",
      "|(150,[0,144,149],...| 42.0|\n",
      "|(150,[39,147,149]...| 42.0|\n",
      "|(150,[1,144,149],...| 35.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = split[1]\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We will use a linear regression model and fit the dataset which we selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(rquery, {lr.regParam:0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "we show a Spark dataframe we created containing both the original features and labels as well as the predictions our model made. We print the first 10 lines in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(150,[60,144,149]...|110.0| 41.74907470931341|\n",
      "|(150,[71,147,149]...|101.0| 58.05154187284671|\n",
      "|(150,[0,144,149],...|101.0|15.721209114835599|\n",
      "|(150,[55,147,149]...|100.0| 32.15716083133794|\n",
      "|(150,[2,144,149],...|100.0| 4.510098821539601|\n",
      "|(150,[36,144,149]...|100.0|23.745331688256393|\n",
      "|(150,[1,144,149],...| 96.0|10.802946022944319|\n",
      "|(150,[0,144,149],...| 95.0| 9.972133713762286|\n",
      "|(150,[39,147,149]...| 94.0| 37.06083788203532|\n",
      "|(150,[2,144,149],...| 83.0| 5.787671132889225|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(rquery)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutate the model\n",
    "Here we use both RMSE and MAE to evaluate our model. According to these metrics, our model doesn't look so good. We can see in the short sample above that the predictions seem wildly off. Now that all the pieces are in place, it will be easy to continue to tweak and refine our model until we meet a goal which we must set for the standard quality of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Root Mean Squared Error = 15.734990564851342\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "RMSE = evaluator.evaluate(predictions)\n",
    "print(\"Model: Root Mean Squared Error = \" + str(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Mean Absolute Error = 9.788315035869392\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\")\n",
    "MAE = evaluator.evaluate(predictions)\n",
    "print(\"Model: Mean Absolute Error = \" + str(MAE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
