{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/convert-pandas-to-pyspark-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.1.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile \n",
    "pd.set_option(\"display.max_columns\",1000)\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "#findspark.init('C:\\Spark\\spark-3.1.1-bin-hadoop2.7')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark df\n",
    "ending sesh wipes the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Project1').getOrCreate()#.master(\"local\") #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW ARE JUST TESTS-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test\n",
    "\"fake\" is just a python dataframe...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>whaterver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  whaterver\n",
       "0      poop\n",
       "1       pee"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = pd.DataFrame(['poop','pee'])\n",
    "fake.columns = ['whaterver']\n",
    "fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"sprk\" is a Spark dataframe made from our python dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- whaterver: string (nullable = true)\n",
      "\n",
      "+---------+\n",
      "|whaterver|\n",
      "+---------+\n",
      "|     poop|\n",
      "|      pee|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "sprk=spark.createDataFrame(fake) \n",
    "sprk.printSchema()\n",
    "sprk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(whaterver='poop')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sprk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|whaterver|\n",
      "+---------+\n",
      "|     poop|\n",
      "|      pee|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sprk.select(\"whaterver\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we make a temporary SQL table called 'fake' within Spark. now we can write SQL queries in spark to find it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|whaterver|\n",
      "+---------+\n",
      "|     poop|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "sprk.createOrReplaceTempView(\"fake\")\n",
    "\n",
    "sqlDF = spark.sql(\"\"\"\n",
    "    SELECT * FROM fake where whaterver like 'po%'\n",
    "    \"\"\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 2\n",
    "has to be a sparkdf first to then create a spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fku</th>\n",
       "      <th>numbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whatever</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who cares</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fku  numbs\n",
       "0        hey      4\n",
       "1   whatever      5\n",
       "2  who cares      6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randf = pd.DataFrame({\n",
    "    'fku': ['hey','whatever','who cares'],\n",
    "    'numbs': [4, 5, 6]\n",
    "})\n",
    "randf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fku: string (nullable = true)\n",
      " |-- numbs: long (nullable = true)\n",
      "\n",
      "+---------+-----+\n",
      "|      fku|numbs|\n",
      "+---------+-----+\n",
      "|      hey|    4|\n",
      "| whatever|    5|\n",
      "|who cares|    6|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "fku=spark.createDataFrame(randf) \n",
    "fku.printSchema()\n",
    "fku.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      fku|numbs|\n",
      "+---------+-----+\n",
      "| whatever|    5|\n",
      "|who cares|    6|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "fku.createOrReplaceTempView(\"fku\")\n",
    "\n",
    "sqlDF = spark.sql(\"\"\"\n",
    "    SELECT * FROM fku where numbs >=5\n",
    "    \"\"\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOVE ARE JUST TESTS -----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Deal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV to python df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY_NAME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_CODE</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>DEP_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N8974C</td>\n",
       "      <td>9E</td>\n",
       "      <td>AVL</td>\n",
       "      <td>Asheville, NC</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1658</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1758</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N922XJ</td>\n",
       "      <td>9E</td>\n",
       "      <td>JFK</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>RDU</td>\n",
       "      <td>Raleigh/Durham, NC</td>\n",
       "      <td>1122</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1255</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N326PQ</td>\n",
       "      <td>9E</td>\n",
       "      <td>CLE</td>\n",
       "      <td>Cleveland, OH</td>\n",
       "      <td>DTW</td>\n",
       "      <td>Detroit, MI</td>\n",
       "      <td>1334</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1417</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N135EV</td>\n",
       "      <td>9E</td>\n",
       "      <td>BHM</td>\n",
       "      <td>Birmingham, AL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1059</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1255</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N914XJ</td>\n",
       "      <td>9E</td>\n",
       "      <td>GTF</td>\n",
       "      <td>Great Falls, MT</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>1057</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1418</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N257NN</td>\n",
       "      <td>MQ</td>\n",
       "      <td>STL</td>\n",
       "      <td>St. Louis, MO</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>1220</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1327</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N855AE</td>\n",
       "      <td>MQ</td>\n",
       "      <td>LGA</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>CMH</td>\n",
       "      <td>Columbus, OH</td>\n",
       "      <td>1048</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>1233</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N688AE</td>\n",
       "      <td>MQ</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>COU</td>\n",
       "      <td>Columbia, MO</td>\n",
       "      <td>2317</td>\n",
       "      <td>52.0</td>\n",
       "      <td>104</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N262NN</td>\n",
       "      <td>MQ</td>\n",
       "      <td>MSN</td>\n",
       "      <td>Madison, WI</td>\n",
       "      <td>ORD</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>N228NN</td>\n",
       "      <td>MQ</td>\n",
       "      <td>DFW</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>LEX</td>\n",
       "      <td>Lexington, KY</td>\n",
       "      <td>1821</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2120</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FL_DATE TAIL_NUM CARRIER ORIGIN       ORIGIN_CITY_NAME DEST  \\\n",
       "0    2019-01-01   N8974C      9E    AVL          Asheville, NC  ATL   \n",
       "1    2019-01-01   N922XJ      9E    JFK           New York, NY  RDU   \n",
       "2    2019-01-01   N326PQ      9E    CLE          Cleveland, OH  DTW   \n",
       "3    2019-01-01   N135EV      9E    BHM         Birmingham, AL  ATL   \n",
       "4    2019-01-01   N914XJ      9E    GTF        Great Falls, MT  MSP   \n",
       "..          ...      ...     ...    ...                    ...  ...   \n",
       "995  2019-01-01   N257NN      MQ    STL          St. Louis, MO  ORD   \n",
       "996  2019-01-01   N855AE      MQ    LGA           New York, NY  CMH   \n",
       "997  2019-01-01   N688AE      MQ    ORD            Chicago, IL  COU   \n",
       "998  2019-01-01   N262NN      MQ    MSN            Madison, WI  ORD   \n",
       "999  2019-01-01   N228NN      MQ    DFW  Dallas/Fort Worth, TX  LEX   \n",
       "\n",
       "         DEST_CITY_NAME DEP_TIME DEP_DELAY ARR_TIME ARR_DELAY CANCELLED  \\\n",
       "0           Atlanta, GA     1658      -7.0     1758     -22.0       0.0   \n",
       "1    Raleigh/Durham, NC     1122      -8.0     1255     -29.0       0.0   \n",
       "2           Detroit, MI     1334      -7.0     1417     -31.0       0.0   \n",
       "3           Atlanta, GA     1059      -1.0     1255      -8.0       0.0   \n",
       "4       Minneapolis, MN     1057      -3.0     1418     -17.0       0.0   \n",
       "..                  ...      ...       ...      ...       ...       ...   \n",
       "995         Chicago, IL     1220      14.0     1327      -7.0       0.0   \n",
       "996        Columbus, OH     1048     -12.0     1233     -34.0       0.0   \n",
       "997        Columbia, MO     2317      52.0      104      80.0       0.0   \n",
       "998         Chicago, IL        0       0.0        0       0.0       1.0   \n",
       "999       Lexington, KY     1821      36.0     2120      32.0       0.0   \n",
       "\n",
       "    CANCELLATION_CODE DIVERTED CARRIER_DELAY WEATHER_DELAY NAS_DELAY  \\\n",
       "0                   0      0.0           0.0           0.0       0.0   \n",
       "1                   0      0.0           0.0           0.0       0.0   \n",
       "2                   0      0.0           0.0           0.0       0.0   \n",
       "3                   0      0.0           0.0           0.0       0.0   \n",
       "4                   0      0.0           0.0           0.0       0.0   \n",
       "..                ...      ...           ...           ...       ...   \n",
       "995                 0      0.0           0.0           0.0       0.0   \n",
       "996                 0      0.0           0.0           0.0       0.0   \n",
       "997                 0      0.0           0.0           0.0      28.0   \n",
       "998                 B      0.0           0.0           0.0       0.0   \n",
       "999                 0      0.0          32.0           0.0       0.0   \n",
       "\n",
       "    SECURITY_DELAY LATE_AIRCRAFT_DELAY Unnamed: 19 DEP_HR  \n",
       "0              0.0                 0.0         0.0     16  \n",
       "1              0.0                 0.0         0.0     11  \n",
       "2              0.0                 0.0         0.0     13  \n",
       "3              0.0                 0.0         0.0     10  \n",
       "4              0.0                 0.0         0.0     10  \n",
       "..             ...                 ...         ...    ...  \n",
       "995            0.0                 0.0         0.0     12  \n",
       "996            0.0                 0.0         0.0     10  \n",
       "997            0.0                52.0         0.0     23  \n",
       "998            0.0                 0.0         0.0         \n",
       "999            0.0                 0.0         0.0     18  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('flights.csv.zip', nrows=1000)\n",
    "df = df.fillna(0)\n",
    "df['DEP_TIME'] = df['DEP_TIME'].astype('int')\n",
    "df['DEP_HR'] = df['DEP_TIME'].astype('str').str[:-2] \n",
    "df['ARR_TIME'] = df['ARR_TIME'].astype('int')\n",
    "df = df.fillna(0)\n",
    "df = df.astype('str')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY_NAME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_CODE</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>DEP_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FL_DATE, TAIL_NUM, CARRIER, ORIGIN, ORIGIN_CITY_NAME, DEST, DEST_CITY_NAME, DEP_TIME, DEP_DELAY, ARR_TIME, ARR_DELAY, CANCELLED, CANCELLATION_CODE, DIVERTED, CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY, Unnamed: 19, DEP_HR]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[df['DEP_HR'] == 'null']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Schema\n",
    "everything is a string at first ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, DateType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FL_DATE\", StringType()),#, DateType()),\n",
    "    StructField(\"TAIL_NUM\", StringType()),\n",
    "    StructField(\"CARRIER\", StringType()),\n",
    "    StructField(\"ORIGIN\", StringType()),\n",
    "    StructField(\"ORIGIN_CITY_NAME\", StringType()),\n",
    "    StructField(\"DEST\", StringType()),\n",
    "    StructField(\"DEST_CITY_NAME\", StringType()),\n",
    "    StructField(\"DEP_TIME\", StringType()),\n",
    "    StructField(\"DEP_DELAY\", StringType()),#, DoubleType()),\n",
    "    StructField(\"ARR_TIME\", StringType()),\n",
    "    StructField(\"ARR_DELAY\", StringType()),#, DoubleType()),\n",
    "    StructField(\"CANCELLED\", StringType()),\n",
    "    StructField(\"CANCELLATION_CODE\", StringType()),\n",
    "    StructField(\"DIVERTED\", StringType()),\n",
    "    StructField(\"CARRIER_DELAY\", StringType()),\n",
    "    StructField(\"WEATHER_DELAY\", StringType()),\n",
    "    StructField(\"NAS_DELAY\", StringType()),\n",
    "    StructField(\"SECURITY_DELAY\", StringType()),\n",
    "    StructField(\"LATE_AIRCRAFT_DELAY\", StringType()),\n",
    "    StructField(\"Unnamed: 19\", StringType()),#, IntegerType())\n",
    "    StructField(\"DEP_HR\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark DF from pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- TAIL_NUM: string (nullable = true)\n",
      " |-- CARRIER: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY: string (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CARRIER_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      " |-- NAS_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- Unnamed: 19: string (nullable = true)\n",
      " |-- DEP_HR: string (nullable = true)\n",
      "\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|    ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 19|DEP_HR|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|       Asheville, NC| ATL|       Atlanta, GA|    1658|     -7.0|    1758|    -22.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    16|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|        New York, NY| RDU|Raleigh/Durham, NC|    1122|     -8.0|    1255|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    11|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|       Cleveland, OH| DTW|       Detroit, MI|    1334|     -7.0|    1417|    -31.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    13|\n",
      "|2019-01-01|  N135EV|     9E|   BHM|      Birmingham, AL| ATL|       Atlanta, GA|    1059|     -1.0|    1255|     -8.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    10|\n",
      "|2019-01-01|  N914XJ|     9E|   GTF|     Great Falls, MT| MSP|   Minneapolis, MN|    1057|     -3.0|    1418|    -17.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    10|\n",
      "|2019-01-01|  N924EV|     9E|   GRB|       Green Bay, WI| DTW|       Detroit, MI|     855|      0.0|    1125|     10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N195PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|     800|     -5.0|     900|    -16.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N319PQ|     9E|   CLT|       Charlotte, NC| JFK|      New York, NY|    1350|    -10.0|    1534|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    13|\n",
      "|2019-01-01|  N933XJ|     9E|   MEM|         Memphis, TN| MSP|   Minneapolis, MN|    1441|     -4.0|    1641|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    14|\n",
      "|2019-01-01|  N933XJ|     9E|   MSP|     Minneapolis, MN| MEM|       Memphis, TN|     847|     -4.0|    1114|     -6.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N8886A|     9E|   ATL|         Atlanta, GA| AEX|    Alexandria, LA|    1856|     -5.0|    1931|    -20.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    18|\n",
      "|2019-01-01|  N302PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|    1427|     -9.0|    1540|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    14|\n",
      "|2019-01-01|  N302PQ|     9E|   ATL|         Atlanta, GA| AGS|       Augusta, GA|    1259|     -6.0|    1343|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    12|\n",
      "|2019-01-01|  N272PQ|     9E|   CVG|      Cincinnati, OH| LGA|      New York, NY|     834|     -6.0|    1022|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N292PQ|     9E|   RDU|  Raleigh/Durham, NC| MCO|       Orlando, FL|    1324|     -1.0|    1504|    -14.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    13|\n",
      "|2019-01-01|  N980EV|     9E|   GSO|Greensboro/High P...| LGA|      New York, NY|    1155|     -5.0|    1340|      3.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    11|\n",
      "|2019-01-01|  N326PQ|     9E|   BIS| Bismarck/Mandan, ND| MSP|   Minneapolis, MN|     809|    124.0|     933|    109.0|      0.0|                0|     0.0|        109.0|          0.0|      0.0|           0.0|                0.0|        0.0|     8|\n",
      "|2019-01-01|  N135EV|     9E|   ATL|         Atlanta, GA| AVL|     Asheville, NC|    1531|     -4.0|    1625|    -10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    15|\n",
      "|2019-01-01|  N8976E|     9E|   LGA|        New York, NY| BTV|    Burlington, VT|    1756|     -4.0|    1913|     -7.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    17|\n",
      "|2019-01-01|  N925XJ|     9E|   PIT|      Pittsburgh, PA| JFK|      New York, NY|    1124|     -1.0|    1249|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|    11|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "flts=spark.createDataFrame(df,schema=schema) \n",
    "flts.printSchema()\n",
    "flts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Delays from String to Doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|    ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 19|DEP_HR|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|       Asheville, NC| ATL|       Atlanta, GA|    1658|     -7.0|    1758|    -22.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  16.0|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|        New York, NY| RDU|Raleigh/Durham, NC|    1122|     -8.0|    1255|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  11.0|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|       Cleveland, OH| DTW|       Detroit, MI|    1334|     -7.0|    1417|    -31.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  13.0|\n",
      "|2019-01-01|  N135EV|     9E|   BHM|      Birmingham, AL| ATL|       Atlanta, GA|    1059|     -1.0|    1255|     -8.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  10.0|\n",
      "|2019-01-01|  N914XJ|     9E|   GTF|     Great Falls, MT| MSP|   Minneapolis, MN|    1057|     -3.0|    1418|    -17.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  10.0|\n",
      "|2019-01-01|  N924EV|     9E|   GRB|       Green Bay, WI| DTW|       Detroit, MI|     855|      0.0|    1125|     10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N195PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|     800|     -5.0|     900|    -16.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N319PQ|     9E|   CLT|       Charlotte, NC| JFK|      New York, NY|    1350|    -10.0|    1534|    -29.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  13.0|\n",
      "|2019-01-01|  N933XJ|     9E|   MEM|         Memphis, TN| MSP|   Minneapolis, MN|    1441|     -4.0|    1641|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  14.0|\n",
      "|2019-01-01|  N933XJ|     9E|   MSP|     Minneapolis, MN| MEM|       Memphis, TN|     847|     -4.0|    1114|     -6.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N8886A|     9E|   ATL|         Atlanta, GA| AEX|    Alexandria, LA|    1856|     -5.0|    1931|    -20.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  18.0|\n",
      "|2019-01-01|  N302PQ|     9E|   AGS|         Augusta, GA| ATL|       Atlanta, GA|    1427|     -9.0|    1540|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  14.0|\n",
      "|2019-01-01|  N302PQ|     9E|   ATL|         Atlanta, GA| AGS|       Augusta, GA|    1259|     -6.0|    1343|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  12.0|\n",
      "|2019-01-01|  N272PQ|     9E|   CVG|      Cincinnati, OH| LGA|      New York, NY|     834|     -6.0|    1022|    -18.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N292PQ|     9E|   RDU|  Raleigh/Durham, NC| MCO|       Orlando, FL|    1324|     -1.0|    1504|    -14.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  13.0|\n",
      "|2019-01-01|  N980EV|     9E|   GSO|Greensboro/High P...| LGA|      New York, NY|    1155|     -5.0|    1340|      3.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  11.0|\n",
      "|2019-01-01|  N326PQ|     9E|   BIS| Bismarck/Mandan, ND| MSP|   Minneapolis, MN|     809|    124.0|     933|    109.0|      0.0|                0|     0.0|        109.0|          0.0|      0.0|           0.0|                0.0|        0.0|   8.0|\n",
      "|2019-01-01|  N135EV|     9E|   ATL|         Atlanta, GA| AVL|     Asheville, NC|    1531|     -4.0|    1625|    -10.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  15.0|\n",
      "|2019-01-01|  N8976E|     9E|   LGA|        New York, NY| BTV|    Burlington, VT|    1756|     -4.0|    1913|     -7.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  17.0|\n",
      "|2019-01-01|  N925XJ|     9E|   PIT|      Pittsburgh, PA| JFK|      New York, NY|    1124|     -1.0|    1249|     -4.0|      0.0|                0|     0.0|          0.0|          0.0|      0.0|           0.0|                0.0|        0.0|  11.0|\n",
      "+----------+--------+-------+------+--------------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "StructType(List(StructField(FL_DATE,StringType,true),StructField(TAIL_NUM,StringType,true),StructField(CARRIER,StringType,true),StructField(ORIGIN,StringType,true),StructField(ORIGIN_CITY_NAME,StringType,true),StructField(DEST,StringType,true),StructField(DEST_CITY_NAME,StringType,true),StructField(DEP_TIME,StringType,true),StructField(DEP_DELAY,DoubleType,true),StructField(ARR_TIME,StringType,true),StructField(ARR_DELAY,DoubleType,true),StructField(CANCELLED,StringType,true),StructField(CANCELLATION_CODE,StringType,true),StructField(DIVERTED,StringType,true),StructField(CARRIER_DELAY,StringType,true),StructField(WEATHER_DELAY,StringType,true),StructField(NAS_DELAY,StringType,true),StructField(SECURITY_DELAY,StringType,true),StructField(LATE_AIRCRAFT_DELAY,StringType,true),StructField(Unnamed: 19,StringType,true),StructField(DEP_HR,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "flts = flts.withColumn(\"DEP_DELAY\", flts['DEP_DELAY'].cast(DoubleType())).withColumn('ARR_DELAY', flts['ARR_DELAY'].cast(DoubleType())).withColumn(\"DEP_HR\", flts['DEP_HR'].cast(DoubleType()))\n",
    "flts.show()\n",
    "print(flts.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flts.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 [20 points]\n",
    "Your first task is to calculate the average flight delays in the dataset. Your supervisor made it\n",
    "clear that you should choose SparkSQL with Python and DataFrames, so that your code should\n",
    "be compatible with other software products of your company. Your deliverables for this task\n",
    "are the following:\n",
    "- A Python file (named “task1.py”) containing the code to produce the desired result.\n",
    "- A report (named “task1.pdf”) explaining the basic intuition of your code.\n",
    "- A screenshot (named “task1.png”) of the produced output (e.g., showing the result in\n",
    "the console). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Departure Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|avg(DEP_DELAY)|\n",
      "+--------------+\n",
      "|         6.596|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT avg(DEP_DELAY)\n",
    "    FROM flights \n",
    "    \"\"\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrival Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|avg(ARR_DELAY)|\n",
      "+--------------+\n",
      "|         0.532|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT avg(ARR_DELAY)\n",
    "    FROM flights \n",
    "    \"\"\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 [35 points]\n",
    "For this task you continue to work with SparkSQL. The objective is to create reports on the\n",
    "average and median departure delays of \n",
    "- (a) all the airports\n",
    "- (b) all the airways in the dataset\n",
    "You should give four reports: \n",
    "- two for the airports (average/median delays)\n",
    "- two for the airways (average/median delays)\n",
    "\n",
    "Each report is a CSV file containing one line for each airport/airway \n",
    "and the lines of each file should be ordered (in descending order) based on the\n",
    "corresponding criterion (average/median delay). \n",
    "No header files are required for these files.\n",
    "An extra instruction you have from your supervisor is that you should take care of some data\n",
    "outliers: you should not consider in your analysis any airports/airways that have extremely\n",
    "low number of flights; the criterion is that any airport/airway belonging in the lowest 1%\n",
    "percentile, regarding the number of flights, should be omitted. Your deliverables for this task\n",
    "are the following:\n",
    "- A Python file (named “task2.py”) containing the code to produce the reports.\n",
    "- A report (named “task2.pdf”) explaining the basic intuition of your code.\n",
    "- The four report files (named “task2-ap-avg.csv”, “task2-ap-med.csv”, “task2-awavg.csv”, and “task2-aw-med.csv”) having the determined file structure. Please\n",
    "restrict the number of lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airports Departure Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    ORIGIN\n",
    "    , avg(DEP_DELAY)\n",
    "    FROM flights \n",
    "    group by ORIGIN\n",
    "    order by 2 desc;\n",
    "    \"\"\")\n",
    "perc = query.stat.approxQuantile(\"avg(DEP_DELAY)\",[0.01],0.0)\n",
    "perc = int(perc[0])\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|ORIGIN|    avg(DEP_DELAY)|\n",
      "+------+------------------+\n",
      "|   TVC|             209.0|\n",
      "|   VPS|             131.5|\n",
      "|   BIS|             124.0|\n",
      "|   PWM|              87.5|\n",
      "|   BIL|              59.0|\n",
      "|   GTF|              49.0|\n",
      "|   RFD|              46.0|\n",
      "|   TUS|              36.0|\n",
      "|   CMH|34.285714285714285|\n",
      "|   MSN|              34.0|\n",
      "|   XNA|              34.0|\n",
      "|   ELP|              31.5|\n",
      "|   FWA|              31.0|\n",
      "|   BLI|30.666666666666668|\n",
      "|   ABQ|              29.0|\n",
      "|   KOA|              28.0|\n",
      "|   ALB|              27.0|\n",
      "|   AZA|26.333333333333332|\n",
      "|   ONT|              25.0|\n",
      "|   MCI|24.666666666666668|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT ORIGIN\n",
    "    , avg(DEP_DELAY)\n",
    "    FROM flights \n",
    "    group by ORIGIN\n",
    "    having avg(DEP_DELAY) > {}\n",
    "    order by 2 desc\n",
    "    limit 100;\n",
    "    \"\"\".format(perc))\n",
    "query.repartition(1).write.csv(\"task2-ap-avg.csv\", sep=',',header=None)\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT ORIGIN\n",
    "    ,percentile(DEP_DELAY, 0.5) as med\n",
    "    FROM flights \n",
    "    group by ORIGIN\n",
    "    order by 2 desc;\n",
    "    \"\"\")\n",
    "perc = query.stat.approxQuantile(\"med\",[0.01],0.0)\n",
    "perc = int(perc[0])\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|ORIGIN|  med|\n",
      "+------+-----+\n",
      "|   TVC|209.0|\n",
      "|   VPS|131.5|\n",
      "|   BIS|124.0|\n",
      "|   PWM| 87.5|\n",
      "|   BIL| 59.0|\n",
      "|   GTF| 49.0|\n",
      "|   RFD| 46.0|\n",
      "|   MSN| 34.0|\n",
      "|   XNA| 34.0|\n",
      "|   ELP| 31.5|\n",
      "|   FWA| 31.0|\n",
      "|   ABQ| 29.0|\n",
      "|   KOA| 28.0|\n",
      "|   ALB| 27.0|\n",
      "|   ONT| 25.0|\n",
      "|   SFB| 23.0|\n",
      "|   COU| 23.0|\n",
      "|   AZA| 21.0|\n",
      "|   BQN| 21.0|\n",
      "|   MSO| 21.0|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT ORIGIN\n",
    "    ,percentile(DEP_DELAY, 0.5) as med\n",
    "    FROM flights \n",
    "    group by ORIGIN\n",
    "    having med > {}\n",
    "    order by 2 desc\n",
    "    limit 100;\n",
    "    \"\"\".format(perc))\n",
    "query.repartition(1).write.csv(\"task2-ap-med.csv\", sep=',',header=None)\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airways Departure Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    CARRIER\n",
    "    , avg(DEP_DELAY)\n",
    "    FROM flights \n",
    "    group by CARRIER\n",
    "    order by 2 desc;\n",
    "    \"\"\")\n",
    "perc = query.stat.approxQuantile(\"avg(DEP_DELAY)\",[0.01],0.0)\n",
    "perc = int(perc[0])\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|CARRIER|    avg(DEP_DELAY)|\n",
      "+-------+------------------+\n",
      "|     G4|11.942857142857143|\n",
      "|     MQ|  7.91044776119403|\n",
      "|     AA| 7.249578414839798|\n",
      "|     NK| 4.504273504273504|\n",
      "|     9E|2.6405228758169934|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    CARRIER\n",
    "    , avg(DEP_DELAY)\n",
    "    FROM flights \n",
    "    group by CARRIER\n",
    "    having avg(DEP_DELAY) > {}\n",
    "    order by 2 desc\n",
    "    limit 100;\n",
    "    \"\"\".format(perc))\n",
    "query.repartition(1).write.csv(\"task2-aw-avg.csv\", sep=',',header=None)\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    CARRIER\n",
    "    ,percentile(DEP_DELAY, 0.5) as med\n",
    "    FROM flights \n",
    "    group by CARRIER\n",
    "    order by 2 desc;\n",
    "    \"\"\")\n",
    "perc = query.stat.approxQuantile(\"med\",[0.01],0.0)\n",
    "perc = int(perc[0])\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|CARRIER| med|\n",
      "+-------+----+\n",
      "|     MQ| 0.0|\n",
      "|     AA|-1.0|\n",
      "|     G4|-1.5|\n",
      "|     NK|-3.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    CARRIER\n",
    "    ,percentile(DEP_DELAY, 0.5) as med\n",
    "    FROM flights \n",
    "    group by CARRIER\n",
    "    having med > {}\n",
    "    order by 2 desc\n",
    "    limit 100;\n",
    "    \"\"\".format(perc))\n",
    "query.repartition(1).write.csv(\"task2-aw-med.csv\", sep=',',header=None)\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 [45 points]\n",
    "As a final task, your supervisor assigned to you to investigate if it is possible to train a linear\n",
    "regression model that could predict the departure delay a flight may have by using, \n",
    "as input:\n",
    "- its origin (column “ORIGIN”)\n",
    "- its airways (column “CARRIER”)\n",
    "- its departure time (column “DEP_TIME”)\n",
    "\n",
    "Again you should use Python and DataFrames, this time with **MLlib**\n",
    "\n",
    "You should pay attention to transform the string-based input features using the proper representation\n",
    "format, and you should explain your choices.\n",
    "\n",
    "Special attention should be given to the “DEP_TIME” column: \n",
    "your supervisor told you that you should only consider the corresponding hour of the day \n",
    "- (which, of course, should be transformed in a one-hot representation).\n",
    "\n",
    "For your training and testing workflow you should first remove outliers (see Task 2) \n",
    "and then split your dataset into two parts:\n",
    "- one that will be used for training reasons and it will contain 70% of the entries\n",
    "- a second one containing the remaining entries andwhich will be used for the assessment of the model. \n",
    "\n",
    "No need to implement a more sophisticated evaluation process (e.g., based on k-fold) is required in this phase. \n",
    "\n",
    "Your code should:\n",
    "- (a) prepare the feature vectors\n",
    "- (b) prepare the training and testing datasets\n",
    "- (c) train the model\n",
    "- (d) print on the screen the first 10 predictions, i.e., pairs of feature vectors (in compact format) and predicted outputs, on the screen \n",
    "- (e) evaluate the accuracy of the model and display the corresponding metric on the screen. \n",
    "\n",
    "Your deliverables are the following:\n",
    "- A Python file (named “task3.py”) containing the code of the preprocessing, training, and evaluation phase of your machine learning workflow.\n",
    "- A report (named “task3.pdf”) explaining the basic intuition of your code and your design decisions and assumptions.\n",
    "- A screenshot (named “task3.png”) showing the output of your machine learning workflow (e.g., showing the results in the console). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15  ,  110\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    ORIGIN\n",
    "    , CARRIER\n",
    "    , DEP_HR\n",
    "    , DEP_DELAY\n",
    "    FROM flights \n",
    "    order by DEP_DELAY desc;\n",
    "    \"\"\")\n",
    "perc1 = query.stat.approxQuantile(\"DEP_DELAY\",[0.01],0.0)\n",
    "perc1 = int(perc1[0])\n",
    "perc99 = query.stat.approxQuantile(\"DEP_DELAY\",[0.99],0.0)\n",
    "perc99 = int(perc99[0])\n",
    "print(perc1,' , ',perc99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|ORIGIN|CARRIER|DEP_HR|DEP_DELAY|\n",
      "+------+-------+------+---------+\n",
      "|   TUS|     AA|  21.0|    110.0|\n",
      "|   GTF|     G4|  20.0|    101.0|\n",
      "|   DFW|     AA|  23.0|    101.0|\n",
      "|   CLT|     AA|  11.0|    100.0|\n",
      "|   BLI|     G4|  18.0|    100.0|\n",
      "|   MCI|     AA|   7.0|    100.0|\n",
      "|   PHX|     AA|  11.0|     96.0|\n",
      "|   DFW|     AA|  14.0|     95.0|\n",
      "|   AZA|     G4|  17.0|     94.0|\n",
      "|   PHL|     AA|  null|     93.0|\n",
      "|   CLT|     AA|  13.0|     83.0|\n",
      "|   PHX|     AA|  22.0|     82.0|\n",
      "|   TPA|     AA|  15.0|     81.0|\n",
      "|   ORD|     AA|  21.0|     80.0|\n",
      "|   CLT|     AA|  15.0|     79.0|\n",
      "|   DFW|     AA|  11.0|     77.0|\n",
      "|   RSW|     AA|  18.0|     71.0|\n",
      "|   MIA|     AA|  18.0|     70.0|\n",
      "|   MSN|     MQ|  19.0|     68.0|\n",
      "|   ELP|     MQ|  17.0|     67.0|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    ORIGIN\n",
    "    , CARRIER\n",
    "    , DEP_HR\n",
    "    , DEP_DELAY\n",
    "    FROM flights\n",
    "    where DEP_DELAY between {} and {}\n",
    "    order by DEP_DELAY desc;\n",
    "    \"\"\".format(perc1,perc99))\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep = query.na.drop()\n",
    "#prep.select(\"DEP_HR\").distinct().orderBy(\"DEP_HR\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "ORGindexer = StringIndexer()\\\n",
    ".setInputCol(\"ORIGIN\")\\\n",
    ".setOutputCol(\"ORIGIN_indx\")\n",
    "\n",
    "CARindexer = StringIndexer()\\\n",
    ".setInputCol(\"CARRIER\")\\\n",
    ".setOutputCol(\"CARRIER_indx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "ORGencoder = OneHotEncoder(dropLast=False)\\\n",
    ".setInputCols([\"ORIGIN_indx\"])\\\n",
    ".setOutputCols([\"ORIGIN_encd\"])\n",
    "\n",
    "CARencoder = OneHotEncoder(dropLast=False)\\\n",
    ".setInputCols([\"CARRIER_indx\"])\\\n",
    ".setOutputCols([\"CARRIER_encd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vector_assembler = VectorAssembler()\\\n",
    ".setInputCols([\"ORIGIN_encd\",\"CARRIER_encd\",\"DEP_HR\"])\\\n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "transformation_pipeline = Pipeline()\\\n",
    ".setStages([ORGindexer, CARindexer, ORGencoder, CARencoder, vector_assembler])\n",
    "fitted_pipeline = transformation_pipeline.fit(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+-----------+------------+----------------+-------------+--------------------+\n",
      "|ORIGIN|CARRIER|DEP_HR|DEP_DELAY|ORIGIN_indx|CARRIER_indx|     ORIGIN_encd| CARRIER_encd|            features|\n",
      "+------+-------+------+---------+-----------+------------+----------------+-------------+--------------------+\n",
      "|   TUS|     AA|  21.0|    110.0|       60.0|         0.0|(144,[60],[1.0])|(5,[0],[1.0])|(150,[60,144,149]...|\n",
      "|   GTF|     G4|  20.0|    101.0|       71.0|         3.0|(144,[71],[1.0])|(5,[3],[1.0])|(150,[71,147,149]...|\n",
      "|   DFW|     AA|  23.0|    101.0|        0.0|         0.0| (144,[0],[1.0])|(5,[0],[1.0])|(150,[0,144,149],...|\n",
      "|   BLI|     G4|  18.0|    100.0|       55.0|         3.0|(144,[55],[1.0])|(5,[3],[1.0])|(150,[55,147,149]...|\n",
      "|   CLT|     AA|  11.0|    100.0|        2.0|         0.0| (144,[2],[1.0])|(5,[0],[1.0])|(150,[2,144,149],...|\n",
      "+------+-------+------+---------+-----------+------------+----------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regr = fitted_pipeline.transform(prep)\n",
    "regr.cache()\n",
    "regr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.createOrReplaceTempView(\"regr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(150,[60,144,149]...|110.0|\n",
      "|(150,[71,147,149]...|101.0|\n",
      "|(150,[0,144,149],...|101.0|\n",
      "|(150,[55,147,149]...|100.0|\n",
      "|(150,[2,144,149],...|100.0|\n",
      "|(150,[36,144,149]...|100.0|\n",
      "|(150,[1,144,149],...| 96.0|\n",
      "|(150,[0,144,149],...| 95.0|\n",
      "|(150,[39,147,149]...| 94.0|\n",
      "|(150,[2,144,149],...| 83.0|\n",
      "|(150,[1,144,149],...| 82.0|\n",
      "|(150,[20,144,149]...| 81.0|\n",
      "|(150,[3,144,149],...| 80.0|\n",
      "|(150,[2,144,149],...| 79.0|\n",
      "|(150,[0,144,149],...| 77.0|\n",
      "|(150,[18,144,149]...| 71.0|\n",
      "|(150,[5,144,149],...| 70.0|\n",
      "|(150,[126,148,149...| 68.0|\n",
      "|(150,[67,148,149]...| 67.0|\n",
      "|(150,[2,144,149],...| 66.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rquery = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    features\n",
    "    , DEP_DELAY as label\n",
    "    FROM regr\n",
    "    group by features\n",
    "    , DEP_DELAY;\n",
    "    \"\"\")\n",
    "rquery.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "https://stackoverflow.com/questions/40293970/spark-data-frame-random-splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(150,[0,144,149],...|101.0|\n",
      "|(150,[36,144,149]...|100.0|\n",
      "|(150,[55,147,149]...|100.0|\n",
      "|(150,[60,144,149]...|110.0|\n",
      "|(150,[71,147,149]...|101.0|\n",
      "|(150,[0,144,149],...| 95.0|\n",
      "|(150,[1,144,149],...| 96.0|\n",
      "|(150,[2,144,149],...| 83.0|\n",
      "|(150,[39,147,149]...| 94.0|\n",
      "|(150,[0,144,149],...| 77.0|\n",
      "|(150,[2,144,149],...| 79.0|\n",
      "|(150,[3,144,149],...| 80.0|\n",
      "|(150,[20,144,149]...| 81.0|\n",
      "|(150,[2,144,149],...| 66.0|\n",
      "|(150,[5,144,149],...| 70.0|\n",
      "|(150,[67,148,149]...| 67.0|\n",
      "|(150,[126,148,149...| 68.0|\n",
      "|(150,[10,146,149]...| 59.0|\n",
      "|(150,[16,144,149]...| 58.0|\n",
      "|(150,[93,144,149]...| 58.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split = rquery.randomSplit([0.7, 0.3])\n",
    "train = split[0]\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(150,[2,144,149],...|100.0|\n",
      "|(150,[1,144,149],...| 82.0|\n",
      "|(150,[18,144,149]...| 71.0|\n",
      "|(150,[3,144,149],...| 65.0|\n",
      "|(150,[4,144,149],...| 60.0|\n",
      "|(150,[1,144,149],...| 57.0|\n",
      "|(150,[62,147,149]...| 57.0|\n",
      "|(150,[32,144,149]...| 54.0|\n",
      "|(150,[3,144,149],...| 48.0|\n",
      "|(150,[0,144,149],...| 45.0|\n",
      "|(150,[40,144,149]...| 44.0|\n",
      "|(150,[0,144,149],...| 42.0|\n",
      "|(150,[12,144,149]...| 43.0|\n",
      "|(150,[0,148,149],...| 36.0|\n",
      "|(150,[1,144,149],...| 34.0|\n",
      "|(150,[1,144,149],...| 33.0|\n",
      "|(150,[0,144,149],...| 31.0|\n",
      "|(150,[24,147,149]...| 30.0|\n",
      "|(150,[0,144,149],...| 29.0|\n",
      "|(150,[1,144,149],...| 28.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = split[1]\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit 2 models, using different regularization parameters\n",
    "model = lr.fit(rquery, {lr.regParam:0.0})\n",
    "#modelB = lr.fit(data, {lr.regParam:100.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(150,[60,144,149]...|110.0| 41.74907470931341|\n",
      "|(150,[71,147,149]...|101.0| 58.05154187284671|\n",
      "|(150,[0,144,149],...|101.0|15.721209114835599|\n",
      "|(150,[55,147,149]...|100.0| 32.15716083133794|\n",
      "|(150,[2,144,149],...|100.0| 4.510098821539601|\n",
      "|(150,[36,144,149]...|100.0|23.745331688256393|\n",
      "|(150,[1,144,149],...| 96.0|10.802946022944319|\n",
      "|(150,[0,144,149],...| 95.0| 9.972133713762286|\n",
      "|(150,[39,147,149]...| 94.0| 37.06083788203532|\n",
      "|(150,[2,144,149],...| 83.0| 5.787671132889225|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(rquery)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Root Mean Squared Error = 15.734990564851342\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "RMSE = evaluator.evaluate(predictions)\n",
    "print(\"Model: Root Mean Squared Error = \" + str(RMSE))\n",
    "# ModelA: Root Mean Squared Error = 128.602026843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Mean Absolute Error = 9.788315035869392\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\")\n",
    "MAE = evaluator.evaluate(predictions)\n",
    "print(\"Model: Mean Absolute Error = \" + str(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus question [+5 points]\n",
    "Write down something you liked for this assignment and something you disliked. You get all\n",
    "the points if you write something relevant to the question, regardless of which your opinion\n",
    "is. If your comments are irrelevant or do not provide any feedback you get no points. Your\n",
    "deliverable should be a report (named “bonus.pdf”) containing your feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission instructions & honor code\n",
    "Your code files should be fully replicable and readable (documentation comments are\n",
    "required and appreciated). Your code should work with Spark v.3 and should be ready to be\n",
    "executed (e.g., containing all the required import statements). Code failing to execute or\n",
    "producing wrong results will be penalised. You understand that this is an individual\n",
    "assignment, and as such you must carry it out alone. You may discuss with your fellow\n",
    "students to better understand the tasks/questions but you should not ask them to share their\n",
    "answers with you or to help you by giving you specific advice. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
